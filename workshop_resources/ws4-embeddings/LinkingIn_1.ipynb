{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8rfa2EVehqV"
      },
      "source": [
        "# ⚓️ Linking2Impresso: Connecting your data with Impresso\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/blob/main/workshop_resources/ws4-embeddings/Linking2impresso.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "If something doesn't work, you can [report a problem](https://github.com/impresso/impresso-datalab-notebooks/blob/main/reporting-problems.md).\n",
        "\n",
        "## What is this notebook about?\n",
        "\n",
        "This notebook demonstrates how to \"dock\" an external text collection (bonus: image collection) into the Impresso historical media archive using semantic embeddings. \n",
        "By transforming textual data into high-dimensional vector representations, we can measure conceptual similarity between new materials and the vast Impresso corpus. \n",
        "\n",
        "The workflow illustrates the full pipeline: from **preparing a small example collection**, through **embedding and semantic search via the Impresso API**, to **exploring connections through an interactive network visualization**. \n",
        "The resulting graph allows us to quickly see how our own texts resonate with historical documents, opening pathways for contextualization, cross-referencing, and discovery across linguistic and temporal boundaries.\n",
        "\n",
        "## What you will learn?\n",
        "\n",
        "In summary, we are going to:\n",
        "- Load a custom text collection  \n",
        "- Compute embeddings using Impresso’s model  \n",
        "- Search for similar content in the Impresso archive  \n",
        "- Visualize the matches as an interactive bipartite graph\n",
        "\n",
        "## Useful resources\n",
        "\n",
        "- [Impresso Python Library](https://impresso.github.io/impresso-py/)\n",
        "- [Impresso Huggind Face](https://ipyleaflet.readthedocs.io/en/latest/index.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Run the following cells to install the required package and to connect to Imrpesso API:\n",
        "\n",
        "> If you are working with Google Colab, you may need to restart the kernel. Go to *Runtime* and select *Restart session*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impresso Python package with embeddings search feature\n",
        "\n",
        "!pip install --force-reinstall git+https://github.com/impresso/impresso-py.git@embeddings-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connecting to Impresso API\n",
        "\n",
        "from impresso import connect\n",
        "impresso = connect('https://dev.impresso-project.ch/public-api/v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9U3Xksq40u0"
      },
      "source": [
        "## Embed text and image with Impresso model\n",
        "\n",
        "### Embed text\n",
        "\n",
        "In this exemple, we are using an text from the **Geneva Convention Pieces**. We are **leveraging the Impresso embedding model for embedding our input texts**.\n",
        "*Feel free to replace the example collections with any of your texts of interest*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr8JWStjaGrJ"
      },
      "source": [
        "> It's important that we apply the same embedding model to our input texts and the texts in Impresso's database. Only then any similarites are meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feAYxnBOiGna"
      },
      "outputs": [],
      "source": [
        "parts = [\"Persons in the hands of the enemy are entitled at all times to respect for their life and for their physical and mental integrity.\",\n",
        "         \"Under the first and second Geneva Conventions of 1949, the belligerents must protect the sick, wounded and shipwrecked as well as medical personnel, ambulances and hospitals. All persons protected under these conventions must be given shelter and cared for by the party to the conflict that holds power over them.\",\n",
        "         \"The third Geneva Convention contains detailed rules on the treatment of prisoners of war.\",\n",
        "         \"The fourth Geneva Convention protects civilians in the hands of the enemy, whether in their own or in occupied territory.\",\n",
        "         \"The first Additional Protocol of 1977 supplements the rules applying to international armed conflicts contained in the four Geneva Conventions. It imposes restrictions on the conduct of hostilities; for example, it prohibits attacks against civilians and civilian objects and restricts the means and methods of warfare\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzz_VBUVfy4r"
      },
      "outputs": [],
      "source": [
        "# ---embed and search---\n",
        "\n",
        "embeddings = []\n",
        "matches = []\n",
        "\n",
        "for i, part in enumerate(parts):\n",
        "    embedding = impresso.tools.embed_text(text=part, target=\"text\")\n",
        "    matches.append(impresso.search.find(embedding=embedding, limit=5))\n",
        "    embeddings.append(embedding)\n",
        "    print(f\"retrieved search results for text  {i+1}/{len(parts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---post-processing for convenience---\n",
        "\n",
        "matches_uids = [[datum[\"uid\"] for datum in m.raw[\"data\"]] for m in matches]\n",
        "articles = [[impresso.content_items.get(uid) for uid in uids] for uids in matches_uids]\n",
        "articles = [[article.raw.get(\"title\") + \" \" + article.raw.get(\"transcript\") for article in a] for a in articles]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embed image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J81guiV6MZ5w"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "image_url = \"https://gallica.bnf.fr/iiif/ark:/12148/bpt6k6069079/f2/775,369,1303,887/max/0/default.jpg\"\n",
        "display(Image(url=image_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y18LKN-MT3k"
      },
      "outputs": [],
      "source": [
        "img_embedding = impresso.tools.embed_image(image=image_url, target=\"image\")\n",
        "\n",
        "matches = impresso.images.find(\n",
        "  embedding=img_embedding,\n",
        "  limit=4\n",
        ")\n",
        "\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> For more information on text-image embeddings, see the notebook on [multimodal](https://github.com/impresso/impresso-datalab-notebooks/blob/main/workshop_resources/ws4-embeddings/multimodal_on_radio.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXSGPXxUe2uA"
      },
      "source": [
        "## Exploring the links with graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTq53q3ufGwE"
      },
      "source": [
        "### Bi-partite graph: Overview of input texts and search results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPJAg221n4AS"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Create the network\n",
        "net = Network(height=\"600px\", width=\"100%\", notebook=True, cdn_resources='in_line')\n",
        "\n",
        "# Add input nodes\n",
        "for i, text in enumerate(parts):\n",
        "    net.add_node(f\"I{i}\", label=f\"Input {i+1}\", title=text, color=\"lightblue\", size=25)\n",
        "\n",
        "# Add result nodes and edges\n",
        "for i, res_list in enumerate(articles):\n",
        "    for j, rtext in enumerate(res_list):\n",
        "        node_id = f\"R{i}_{j}\"\n",
        "        url = \"https://dev.impresso-project.ch/app/article/\" + matches_uids[i][j]\n",
        "        title = rtext + f\"<br><a href={url} target='_blank'>Link</a>\"\n",
        "        net.add_node(node_id, label=f\"R{i+1}.{j+1}\", title=title, color=\"lightgreen\")\n",
        "        net.add_edge(f\"I{i}\", node_id)\n",
        "\n",
        "# Save and embed in Colab\n",
        "net.save_graph(\"graph.html\")\n",
        "\n",
        "# Display inline in Colab\n",
        "display(HTML(\"graph.html\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmUtWjdofNrB"
      },
      "source": [
        "### Similarity Graph: Revealing additional relations, and relation strengths\n",
        "\n",
        "The first graph above is a *\"discrete graph\"*: it only showed direct matches for each input document. Now we want a more connected representation, with:\n",
        "- Weighted links that reflect the strength of similarity\n",
        "- Connections among the retrieved documents themselves\n",
        "\n",
        "To achieve this, we construct a softer graph that:\n",
        "- Captures similarity relations between any input texts\n",
        "- And any texts among the search results\n",
        "\n",
        "For proper display (closeness of nodes), we rely on the help of **UMAP** (Uniform Manifold Approximation and Projection). It's a **dimensionality reduction technique** that projects high-dimensional data - like embeddings, into 2D or 3D space while preserving the data’s underlying structure and similarity relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evr73LiWNfLL"
      },
      "source": [
        "> To compute similarities, we first convert Impresso’s string-encoded embeddings into usable numeric vectors, then apply our functions to map between the string format and vector space in both directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90thY2FMfnTG"
      },
      "outputs": [],
      "source": [
        "# Converting string-encoded embeddings to numeric vectors\n",
        "\n",
        "import base64\n",
        "import struct\n",
        "\n",
        "# --- our handy mapping functions ---\n",
        "def string2vector(embedding_string):\n",
        "    # convert base64 string to a float array\n",
        "    _, arr = embedding_string.split(':')\n",
        "    arr = base64.b64decode(arr)\n",
        "    embedding_vector = [struct.unpack('f', arr[i:i+4])[0] for i in range(0, len(arr), 4)]\n",
        "    return embedding_vector\n",
        "\n",
        "# the inverse function. Not used, just for completeness sake\n",
        "def vector2string(vec, prefix=\"gte-768\"):\n",
        "    # pack floats into bytes\n",
        "    arr = b''.join(struct.pack('f', x) for x in vec)\n",
        "    # encode bytes to base64 string\n",
        "    encoded = base64.b64encode(arr).decode('utf-8')\n",
        "    # return in same format as original (\"prefix:encoded_string\")\n",
        "    return f\"{prefix}:{encoded}\"\n",
        "\n",
        "# --- applying the mapping functions ---\n",
        "# first we get the embeddings for the matches (in Impresso format, note that we already have the embedding for the input)\n",
        "matches_embeddings = [[impresso.content_items.get_embeddings(uid)[0] for uid in match_uid] for match_uid in matches_uids]\n",
        "# map input embedding string format to vector\n",
        "part_embeddings = [string2vector(emb) for emb in embeddings]\n",
        "# map match embedding string format to vector\n",
        "article_embeddings = [[string2vector(emb) for emb in embs] for embs in matches_embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AEjMTtvutXI"
      },
      "outputs": [],
      "source": [
        "# Buidling an embedding similarity graph\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pyvis.network import Network\n",
        "import umap\n",
        "\n",
        "# Flatten results for convenience\n",
        "flat_article_embeddings = np.vstack(article_embeddings)\n",
        "flat_articles = [r for sublist in articles for r in sublist]\n",
        "flat_uris = [None] * len(part_embeddings) + [r for sublist in matches_uids for r in sublist]\n",
        "\n",
        "# Combine everything for similarity computation\n",
        "all_embeddings = np.vstack([part_embeddings, flat_article_embeddings])\n",
        "all_texts = parts + flat_articles\n",
        "node_types = ['input'] * len(part_embeddings) + ['result'] * len(flat_article_embeddings)\n",
        "\n",
        "# Dimensionality reduction (UMAP)\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "coords = reducer.fit_transform(all_embeddings)\n",
        "\n",
        "# Compute cosine similarities\n",
        "sim_matrix = cosine_similarity(all_embeddings)\n",
        "\n",
        "#  Build the PyVis network\n",
        "net = Network(height=\"700px\", width=\"100%\", notebook=True, cdn_resources='in_line')\n",
        "scale = 150\n",
        "# Add nodes\n",
        "for i, (text, ntype) in enumerate(zip(all_texts, node_types)):\n",
        "    color = \"lightblue\" if ntype == \"input\" else \"lightgreen\"\n",
        "    size = 25 if ntype == \"input\" else 15\n",
        "    label = f\"I{i+1}\" if ntype == \"input\" else f\"R{i - len(flat_articles) + 1}\"\n",
        "    x, y = coords[i] * scale  # scale coordinates\n",
        "    url = None if ntype == \"input\" else \"https://dev.impresso-project.ch/app/article/\" + flat_uris[i]\n",
        "    title = text if ntype == \"input\" else text + f\"<br><a href={url} target='_blank'>Link</a>\"\n",
        "    net.add_node(\n",
        "        i,\n",
        "        label=label,\n",
        "        title=title,\n",
        "        color=color,\n",
        "        size=size,\n",
        "        x=float(x),\n",
        "        y=float(y),\n",
        "        fixed={'x': True, 'y': True}  # properly fix both axes\n",
        "    )\n",
        "\n",
        "# Add edges based on similarity\n",
        "threshold = 0.7  # Adjust to make the graph denser/sparser\n",
        "for i in range(len(all_embeddings)):\n",
        "    for j in range(i + 1, len(all_embeddings)):\n",
        "        sim = float(sim_matrix[i, j])\n",
        "        if sim > threshold:\n",
        "            net.add_edge(i, j, value=sim, title=f\"Similarity: {sim:.2f}\")\n",
        "\n",
        "\n",
        "# --- Display inline in Colab ---\n",
        "net.set_options(\"\"\"\n",
        "{\n",
        "  \"physics\": {\n",
        "    \"enabled\": false\n",
        "  }\n",
        "}\n",
        "\"\"\")\n",
        "net.save_graph(\"embedding_similarity_graph.html\")\n",
        "display(HTML(\"embedding_similarity_graph.html\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57l7n4KZNJ0G"
      },
      "source": [
        "#### Visualizing the result as a graph\n",
        "\n",
        "Just as we built a connected network for input texts and their matches in Impresso, we can do the same for the input image and related images—entirely based on the semantic similarity of their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBZy5--k0YHx"
      },
      "outputs": [],
      "source": [
        "# --pre-processing-- for convenience\n",
        "matches_uids = [datum[\"uid\"] for datum in matches.raw[\"data\"]]\n",
        "links_uids = [datum[\"contentItemUid\"] for datum in matches.raw[\"data\"]]\n",
        "matches_embeddings = [impresso.images.get_embeddings(uid)[0] for uid in matches_uids]\n",
        "part_embeddings = [string2vector(emb) for emb in [embedding]]\n",
        "article_embeddings = [[string2vector(emb) for emb in embs] for embs in [matches_embeddings]]\n",
        "matches_uids = [matches_uids]\n",
        "links_uids = [links_uids]\n",
        "\n",
        "flat_article_embeddings = np.vstack(article_embeddings)\n",
        "flat_articles = [r for sublist in articles for r in sublist]\n",
        "flat_uris = [None] * len(part_embeddings) + [r for sublist in links_uids for r in sublist]\n",
        "\n",
        "# Combine everything for similarity computation\n",
        "all_embeddings = np.vstack([part_embeddings, flat_article_embeddings])\n",
        "all_texts = parts + flat_articles\n",
        "node_types = ['input'] * len(part_embeddings) + ['result'] * len(flat_article_embeddings)\n",
        "\n",
        "# Dimensionality reduction (UMAP)\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "coords = reducer.fit_transform(all_embeddings)\n",
        "\n",
        "# Compute cosine similarities\n",
        "sim_matrix = cosine_similarity(all_embeddings)\n",
        "\n",
        "#  Build the PyVis network\n",
        "net = Network(height=\"700px\", width=\"100%\", notebook=True, cdn_resources='in_line')\n",
        "scale = 150\n",
        "# Add nodes\n",
        "for i, (text, ntype) in enumerate(zip(all_texts, node_types)):\n",
        "    color = \"lightblue\" if ntype == \"input\" else \"lightgreen\"\n",
        "    size = 25 if ntype == \"input\" else 15\n",
        "    label = f\"I{i+1}\" if ntype == \"input\" else f\"R{i - len(flat_articles) + 1}\"\n",
        "    x, y = coords[i] * scale  # scale coordinates\n",
        "    url = None if ntype == \"input\" else \"https://dev.impresso-project.ch/app/article/\" + flat_uris[i]\n",
        "    title = \"Input Image\" if ntype == \"input\" else \"Impresso Image\" + f\"<br><a href={url} target='_blank'>Link</a>\"\n",
        "    net.add_node(\n",
        "        i,\n",
        "        label=label,\n",
        "        title=title,\n",
        "        color=color,\n",
        "        size=size,\n",
        "        x=float(x),\n",
        "        y=float(y),\n",
        "        fixed={'x': True, 'y': True}  # properly fix both axes\n",
        "    )\n",
        "\n",
        "# Add edges based on similarity\n",
        "threshold = 0.0  # Adjust to make the graph denser/sparser\n",
        "for i in range(len(all_embeddings)):\n",
        "    for j in range(i + 1, len(all_embeddings)):\n",
        "        sim = float(sim_matrix[i, j])\n",
        "        if sim > threshold:\n",
        "            net.add_edge(i, j, value=sim, title=f\"Similarity: {sim:.2f}\")\n",
        "\n",
        "# --- Display inline in Colab ---\n",
        "net.set_options(\"\"\"\n",
        "{\n",
        "  \"physics\": {\n",
        "    \"enabled\": false\n",
        "  }\n",
        "}\n",
        "\"\"\")\n",
        "net.save_graph(\"embedding_similarity_graph.html\")\n",
        "display(HTML(\"embedding_similarity_graph.html\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUrMNOzOvUqk"
      },
      "source": [
        "### Externally applying the embedding model\n",
        "\n",
        "In the final cells of this notebook, we show how to load an external embedding model to embed any texts. As an example, we download the same model used in Impresso from its original source and verify that it produces the same embeddings as those returned by Impresso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCo9qxDB5pUk"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install sentence-transformers>=3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqBqGpP2vlra"
      },
      "outputs": [],
      "source": [
        "# The first cell of revealing similarities and links can be replaced with this:\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name_or_path=\"Alibaba-NLP/gte-multilingual-base\"\n",
        "model = SentenceTransformer(model_name_or_path, trust_remote_code=True)\n",
        "part_embeddings = model.encode(parts, normalize_embeddings=True)\n",
        "article_embeddings = [model.encode(articleset, normalize_embeddings=True) for articleset in articles]\n",
        "\n",
        "# verify that the embedding model works as intended:\n",
        "print(model.encode([\"hello\"], normalize_embeddings=True)[0][:4])\n",
        "print(string2vector(impresso.tools.embed_text(text=\"hello\", target=\"text\"))[:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we learned **how to connect external texts and images to the Impresso ecosystem through embeddings**. \n",
        "We showed how to embed user-provided inputs with the same model used internally by Impresso, making it possible to compare personal data with historical content in the collection. \n",
        "We explored two types of visualisation: a **bipartite graph** that highlights direct matches between inputs and search results, and a **similarity graph** that reveals deeper relational structures, strengths of connections, and latent proximity among retrieved items. \n",
        "In the bonus, we demonstrated how to load the embedding model locally to reproduce the same representations outside the platform. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Project and License info\n",
        "\n",
        "### Notebook credits [![CreditLogo.png](data:image/png;base64,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)](https://credit.niso.org/)\n",
        "\n",
        "**Writing - Original draft:**  Roman Kalyakin. **Conceptualization:** Marten Düring. **Software:** Roman Kalyakin. **Writing - Review & Editing**: Juri Opitz, Simon Clematide, Cao Vy. **Validation:** Maud Ehrmann, Kirill Veprikov. **Datalab editorial board:** Caio Mello (Managing), Pauline Conti, Emanuela Boros, Marten Düring, Juri Opitz, Martin Grandjean, Estelle Bunout, Cao Vy. **Data curation & Formal analysis:** Maud Ehrmann, Emanuela Boros, Pauline Conti, Simon Clematide, Juri Opitz, Andrianos Michail. **Methodology:** Roman Kalyakin. **Supervision:** Marten Düring. **Funding aquisition:** Maud Ehrmann, Simon Clematide, Marten Düring, Raphaëlle Ruppen Coutaz.\n",
        "\n",
        "<br><a target=\"_blank\" href=\"https://creativecommons.org/licenses/by/4.0/\">\n",
        "  <img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by.png\"  width=\"100\" alt=\"Open In Colab\"/>\n",
        "</a> \n",
        "\n",
        "This notebook is published under [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0/)\n",
        "\n",
        "For feedback on this notebook, please send an email to info@impresso-project.ch\n",
        "\n",
        "### Impresso project\n",
        "\n",
        "[Impresso - Media Monitoring of the Past](https://impresso-project.ch) is an interdisciplinary research project that aims to develop and consolidate tools for processing and exploring large collections of media archives across modalities, time, languages and national borders. The first project (2017-2021) was funded by the Swiss National Science Foundation under grant No. [CRSII5_173719](http://p3.snf.ch/project-173719) and the second project (2023-2027) by the SNSF under grant No. [CRSII5_213585](https://data.snf.ch/grants/grant/213585) and the Luxembourg National Research Fund under grant No. 17498891.\n",
        "<br></br>\n",
        "### License\n",
        "\n",
        "All Impresso code is published open source under the [GNU Affero General Public License](https://github.com/impresso/impresso-pyindexation/blob/master/LICENSE) v3 or later.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/impresso/impresso.github.io/blob/master/assets/images/3x1--Yellow-Impresso-Black-on-White--transparent.png?raw=true\" width=\"350\" alt=\"Impresso Project Logo\"/>\n",
        "</p>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
