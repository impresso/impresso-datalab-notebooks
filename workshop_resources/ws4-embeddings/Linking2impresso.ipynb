{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR4045u-NGsZ"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/blob/main/workshop_resources/ws4-embeddings/Linking2impresso.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8rfa2EVehqV"
      },
      "source": [
        "# ‚öìÔ∏è Linking2Impresso: Connecting Your Data with Impresso\n",
        "\n",
        "## What is this notebook about?\n",
        "\n",
        "This notebook shows how to link an external collection of texts (and optionally images) to the Impresso historical media archive using semantic embeddings.  \n",
        "It walks through preparing a small example dataset, generating embeddings, querying the\n",
        "Impresso API for similar items, and visualising the results in interactive semantic\n",
        "graphs.\n",
        "\n",
        "## Why is this useful?\n",
        "\n",
        "Semantic linking helps researchers contextualise their own materials within a large historical corpus. By comparing your texts with Impresso documents, you can identify related themes, trace conceptual connections, and explore the broader historical discourse.  \n",
        "This notebook shows how embedding-based search offers an efficient way to connect local\n",
        "datasets to digital archives.\n",
        "\n",
        "## How does it work?\n",
        "\n",
        "The workflow is straightforward:\n",
        "\n",
        "1. Prepare or upload your text collection.\n",
        "2. Generate embeddings with the Impresso embedding model.\n",
        "3. Query the Impresso API using vector-based similarity search.\n",
        "4. Visualize input texts and retrieved items using discrete and similarity graphs.\n",
        "\n",
        "Impresso stores text and image embeddings in a compact Base64-encoded format.  \n",
        "For local similarity computations, these embeddings must be decoded into numeric vectors.  \n",
        "The notebook provides helper functions to convert between Impresso‚Äôs internal format and standard floating-point vectors.\n",
        "\n",
        "Both text and image embeddings follow the same principle but rely on different\n",
        "underlying models.\n",
        "\n",
        "## What will you learn?\n",
        "\n",
        "In this notebook, you will learn how to:\n",
        "\n",
        "- Load or provide your own text collection.\n",
        "- Generate text embeddings using the Impresso API.\n",
        "- Perform semantic similarity search against the Impresso archive.\n",
        "- Retrieve and inspect matched historical documents.\n",
        "- Build and interpret two types of interactive semantic graphs.\n",
        "- Optionally embed and link your own images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-xPtRYr5c7A"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Install the necessary Python packages used throughout this notebook.\n",
        "\n",
        "Note: The `impresso-py` installation uses a specific branch that provides the embedding-based search functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v7LXx1ypfTwA"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/impresso/impresso-py.git@embeddings-search\n",
        "%pip install pyvis scikit-learn umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9U3Xksq40u0"
      },
      "source": [
        "## Input Data: Example Collections\n",
        "\n",
        "This notebook includes two small example datasets.  \n",
        "One consists of French Assembly notes, and the other contains short English texts related to the Geneva Conventions.\n",
        "\n",
        "The French set is kept for experimentation but is not used by default.  \n",
        "The English set serves as the main example because the Impresso embedding model performs reliably on English input.\n",
        "\n",
        "You may replace these samples with your own files, paste text directly, or upload documents through Colab/Drive.\n",
        "\n",
        "### Example Collection I: French Assembly Notes\n",
        "\n",
        "_This dataset is truncated for demonstration purposes. The goal is simply to illustrate how to prepare a collection._\n",
        "\n",
        "### Example Collection II: Geneva Convention Pieces\n",
        "\n",
        "_This English dataset is used in the remainder of this notebook as our running example._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zef4MfukYV2_"
      },
      "source": [
        "### Example Collection I: French Assembly Notes\n",
        "\n",
        "This is an example collection provided by Martin Grandjean.\n",
        "\n",
        "It is **French texts**, so we do not use this here by default. But leave it for further experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP0JpM8Yc-Jc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "Collectionlink=\"https://gist.githubusercontent.com/flipz357/e22f1f9df1b263d29927ec21440daeab/raw/199869d9ffce9b4121f7a97f00dd57eec4c8e763/gistfile1.txt\"\n",
        "wget $Collectionlink -O collection.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW0qNvsyfLZv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "with open(\"collection.md\", \"r\") as f:\n",
        "    collection = f.read()\n",
        "parts, titles = re.split(\"^#.*$\", collection, flags=re.MULTILINE), [\n",
        "    \"notitle\"\n",
        "] + re.findall(\"^#.*$\", collection, flags=re.MULTILINE)\n",
        "parts = [part.replace(\"\\n\", \" \")[:200] for part in parts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s9iJQTKY5DV"
      },
      "source": [
        "### Example Collection II: Geneva Convention Pieces\n",
        "\n",
        "This is an example collection of **English Text**, and we thus use it as the basis for our data. It is fairly short and simple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feAYxnBOiGna"
      },
      "outputs": [],
      "source": [
        "parts = [\n",
        "    (\n",
        "        \"Persons in the hands of the enemy are entitled at all times to respect for\"\n",
        "        \" their life and for their physical and mental integrity.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Under the first and second Geneva Conventions of 1949, the belligerents must\"\n",
        "        \" protect the sick, wounded and shipwrecked as well as medical personnel,\"\n",
        "        \" ambulances and hospitals. All persons protected under these conventions must\"\n",
        "        \" be given shelter and cared for by the party to the conflict that holds power\"\n",
        "        \" over them.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The third Geneva Convention contains detailed rules on the treatment of\"\n",
        "        \" prisoners of war.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The fourth Geneva Convention protects civilians in the hands of the enemy,\"\n",
        "        \" whether in their own or in occupied territory.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The first Additional Protocol of 1977 supplements the rules applying to\"\n",
        "        \" international armed conflicts contained in the four Geneva Conventions. It\"\n",
        "        \" imposes restrictions on the conduct of hostilities; for example, it prohibits\"\n",
        "        \" attacks against civilians and civilian objects and restricts the means and\"\n",
        "        \" methods of warfare\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq9yqBmjZ820"
      },
      "source": [
        "## Initialising an Impresso Session\n",
        "\n",
        "We now establish a connection to the Impresso API.  \n",
        "The endpoint used here is the development API suitable for demonstrations.  \n",
        "If you work with privileged or production data, you may need to provide an API token and adjust the base URL accordingly.\n",
        "\n",
        "The session object gives access to several components:\n",
        "\n",
        "- `tools` for embedding text or images\n",
        "- `search` for semantic retrieval\n",
        "- `content_items` for fetching metadata and full texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjMtK_vOj7U_"
      },
      "outputs": [],
      "source": [
        "from impresso import connect\n",
        "\n",
        "impresso_session = connect(\"https://dev.impresso-project.ch/public-api/v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muT7wHa0aCHf"
      },
      "source": [
        "## Embedding the Input Texts and Semantic Search\n",
        "\n",
        "To link our documents with the Impresso archive, we first generate embeddings for each input text using the Impresso text model.  \n",
        "Semantic search requires that both the query embeddings and the archive embeddings originate from the same model.\n",
        "\n",
        "For each input text, we:\n",
        "\n",
        "1. Compute an embedding.\n",
        "2. Query the Impresso archive for the most similar documents.\n",
        "3. Collect the results for further inspection and visualisation.\n",
        "\n",
        "If your texts are very short or highly domain-specific, the retrieved matches may vary in quality or be sparse.  \n",
        "Increasing the `limit` parameter may be useful in such cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzz_VBUVfy4r"
      },
      "outputs": [],
      "source": [
        "# ---embed and search---\n",
        "\n",
        "# this is to collect the embeddings of our input texts\n",
        "embeddings = []\n",
        "# this is to collect the search results\n",
        "matches = []\n",
        "for i, part in enumerate(parts):\n",
        "    # embedding one of your documents\n",
        "    embedding = impresso_session.tools.embed_text(text=part, target=\"text\")\n",
        "    # using the embedding to search in impresso\n",
        "    matches.append(impresso_session.search.find(embedding=embedding, limit=5))\n",
        "    # storing embedding for later\n",
        "    embeddings.append(embedding)\n",
        "    print(f\"retrieved search results for text  {i+1}/{len(parts)}\")\n",
        "\n",
        "# ---some post-processing for convenience---\n",
        "\n",
        "# get ids of search results\n",
        "matches_uids = [[datum[\"uid\"] for datum in m.raw[\"data\"]] for m in matches]\n",
        "# get texts of search results\n",
        "articles = [\n",
        "    [impresso_session.content_items.get(uid) for uid in uids] for uids in matches_uids\n",
        "]\n",
        "# concatenate title and text of search results\n",
        "articles = [\n",
        "    [article.raw.get(\"title\") + \" \" + article.raw.get(\"transcript\") for article in a]\n",
        "    for a in articles\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-processing Retrieved Matches\n",
        "\n",
        "The semantic search returns a list of content item identifiers.  \n",
        "To analyse the results, we fetch each item‚Äôs title and transcript and combine them into short previews.  \n",
        "Note that OCR quality varies across newspapers in Impresso; retrieved text may occasionally include OCR noise.\n",
        "\n",
        "This post-processing step prepares the data for visualisation and supports quick manual inspection of semantic matches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXSGPXxUe2uA"
      },
      "source": [
        "## Exploring the Links: Bipartite Overview Graph\n",
        "\n",
        "The first visualisation is a bipartite graph connecting each input text with the documents returned by semantic search.  \n",
        "Input texts and retrieved items are displayed as two node types, using colours to distinguish them.\n",
        "\n",
        "This representation helps you see, at a glance:\n",
        "\n",
        "- which Impresso items are linked to each input text\n",
        "- how many results each query retrieves\n",
        "- how input data clusters conceptually\n",
        "\n",
        "Hovering over a node reveals a tooltip with the text preview and a link to the corresponding item in the Impresso interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTq53q3ufGwE"
      },
      "source": [
        "### üï∏Ô∏è: **Discrete Graph**: Overview of input texts and search results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPJAg221n4AS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Create the network\n",
        "net = Network(height=\"600px\", width=\"100%\", notebook=True, cdn_resources=\"in_line\")\n",
        "\n",
        "# Add input nodes\n",
        "for i, text in enumerate(parts):\n",
        "    net.add_node(f\"I{i}\", label=f\"Input {i+1}\", title=text, color=\"lightblue\", size=25)\n",
        "\n",
        "# Add result nodes and edges\n",
        "for i, res_list in enumerate(articles):\n",
        "    for j, rtext in enumerate(res_list):\n",
        "        node_id = f\"R{i}_{j}\"\n",
        "        url = \"https://dev.impresso-project.ch/app/article/\" + matches_uids[i][j]\n",
        "        title = rtext + f\"<br><a href={url} target='_blank'>Link</a>\"\n",
        "        net.add_node(node_id, label=f\"R{i+1}.{j+1}\", title=title, color=\"lightgreen\")\n",
        "        net.add_edge(f\"I{i}\", node_id)\n",
        "\n",
        "# Save\n",
        "net.save_graph(\"graph.html\")\n",
        "abs_path = os.path.abspath(\"graph.html\")\n",
        "print(f\"Graph saved to {abs_path}\")\n",
        "# Display inline in Colab\n",
        "display(HTML(\"graph.html\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmUtWjdofNrB"
      },
      "source": [
        "## Similarity Graph: Structure Beyond Direct Matches\n",
        "\n",
        "Whereas the bipartite graph shows only input‚Üíresult connections,  \n",
        "the similarity graph reveals relationships among all documents: inputs and retrieved results alike.\n",
        "\n",
        "This graph is constructed as follows:\n",
        "\n",
        "- All embeddings are mapped into a 2D space using UMAP.\n",
        "- Cosine similarity is computed between every pair of documents.\n",
        "- Edges are drawn only when the similarity exceeds a chosen threshold.\n",
        "\n",
        "UMAP provides an approximate visual clustering of documents.  \n",
        "Its layout is non-deterministic by nature, so small variations between runs are expected.  \n",
        "For very small datasets, the projection may appear unstable; this is normal behaviour.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evr73LiWNfLL"
      },
      "source": [
        "#### **Before we start**: Handy functions for mapping Impresso embeddings to vector\n",
        "\n",
        "- We need vectors to compute the similarity function\n",
        "- Impresso embeddings are encoded with a specialized string format\n",
        "- So we build and apply functions that map from the string format to an actual vector, and back\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90thY2FMfnTG"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import struct\n",
        "\n",
        "\n",
        "# --- our handy mapping functions ---\n",
        "def string2vector(embedding_string):\n",
        "    # convert base64 string to a float array\n",
        "    _, arr = embedding_string.split(\":\")\n",
        "    arr = base64.b64decode(arr)\n",
        "    embedding_vector = [\n",
        "        struct.unpack(\"f\", arr[i : i + 4])[0] for i in range(0, len(arr), 4)\n",
        "    ]\n",
        "    return embedding_vector\n",
        "\n",
        "\n",
        "# the inverse function. Not used, just for completeness sake\n",
        "def vector2string(vec, prefix=\"gte-768\"):\n",
        "    # pack floats into bytes\n",
        "    arr = b\"\".join(struct.pack(\"f\", x) for x in vec)\n",
        "    # encode bytes to base64 string\n",
        "    encoded = base64.b64encode(arr).decode(\"utf-8\")\n",
        "    # return in same format as original (\"prefix:encoded_string\")\n",
        "    return f\"{prefix}:{encoded}\"\n",
        "\n",
        "\n",
        "# --- applying the mapping functions ---\n",
        "# first we get the embeddings for the matches (in Impresso format, note that we already have the embedding for the input)\n",
        "matches_embeddings = [\n",
        "    [impresso_session.content_items.get_embeddings(uid)[0] for uid in match_uid]\n",
        "    for match_uid in matches_uids\n",
        "]\n",
        "# map input embedding string format to vector\n",
        "part_embeddings = [string2vector(emb) for emb in embeddings]\n",
        "# map match embedding string format to vector\n",
        "article_embeddings = [\n",
        "    [string2vector(emb) for emb in embs] for embs in matches_embeddings\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMYmTHMGN4Ui"
      },
      "source": [
        "#### Build the similarity graph\n",
        "\n",
        "Idea:\n",
        "\n",
        "- The graph above is a _\"discrete graph\"_:\n",
        "  - It shows the matches for every input document\n",
        "- Now we would like to build a graph that is more connected:\n",
        "  - Strength of the connections\n",
        "  - Show connectivity _among the returned documents_\n",
        "- So this time we build a softer graph\n",
        "  - Captures similarity relations between any input texts,\n",
        "  - And any texts among the search results\n",
        "\n",
        "For proper display (closeness of nodes), we rely on the help of UMAP.\n",
        "\n",
        "- UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique that projects high-dimensional data (like embeddings) into 2D or 3D space while preserving the data‚Äôs underlying structure and similarity relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AEjMTtvutXI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pyvis.network import Network\n",
        "import umap\n",
        "\n",
        "\n",
        "def prepare_flat_results(article_embeddings, articles, matches_uids):\n",
        "    \"\"\"\n",
        "    Flatten nested embeddings, texts, and UIDs and ensure alignment.\n",
        "    \"\"\"\n",
        "    flat_embs = np.vstack(article_embeddings)\n",
        "    flat_texts = [txt for group in articles for txt in group]\n",
        "    flat_uris = [uid for group in matches_uids for uid in group]\n",
        "\n",
        "    assert len(flat_embs) == len(flat_texts) == len(flat_uris), \"Alignment error\"\n",
        "    return flat_embs, flat_texts, flat_uris\n",
        "\n",
        "\n",
        "def reduce_to_2d(embeddings, metric=\"cosine\", random_state=42):\n",
        "    \"\"\"\n",
        "    Apply UMAP to reduce embeddings to 2D coordinates.\n",
        "    \"\"\"\n",
        "    reducer = umap.UMAP(n_components=2, metric=metric, random_state=random_state)\n",
        "    return reducer.fit_transform(embeddings)\n",
        "\n",
        "\n",
        "def build_graph(\n",
        "    embeddings,\n",
        "    texts,\n",
        "    node_types,\n",
        "    uris,\n",
        "    coords,\n",
        "    threshold=0.7,\n",
        "    outfile=\"embedding_similarity_graph.html\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Build and save a PyVis graph from embeddings, metadata and 2D coordinates.\n",
        "    \"\"\"\n",
        "    net = Network(height=\"700px\", width=\"100%\", notebook=True, cdn_resources=\"in_line\")\n",
        "\n",
        "    # Scale node coordinates to visible ranges\n",
        "    scale = float(np.std(coords) * 150) or 150.0\n",
        "\n",
        "    result_idx = 1\n",
        "    for i, (text, ntype) in enumerate(zip(texts, node_types)):\n",
        "        is_input = ntype == \"input\"\n",
        "        color = \"lightblue\" if is_input else \"lightgreen\"\n",
        "        size = 25 if is_input else 15\n",
        "        label = f\"I{i+1}\" if is_input else f\"R{result_idx}\"\n",
        "        if not is_input:\n",
        "            result_idx += 1\n",
        "\n",
        "        x, y = coords[i] * scale\n",
        "        url = (\n",
        "            None\n",
        "            if is_input\n",
        "            else f\"https://dev.impresso-project.ch/app/article/{uris[i]}\"\n",
        "        )\n",
        "        title = (\n",
        "            text if is_input else text + f\"<br><a href={url} target='_blank'>Link</a>\"\n",
        "        )\n",
        "\n",
        "        net.add_node(\n",
        "            i,\n",
        "            label=label,\n",
        "            title=title,\n",
        "            color=color,\n",
        "            size=size,\n",
        "            x=float(x),\n",
        "            y=float(y),\n",
        "            fixed={\"x\": True, \"y\": True},\n",
        "        )\n",
        "\n",
        "    # Add edges from cosine similarity\n",
        "    sim_matrix = cosine_similarity(embeddings)\n",
        "    for i in range(len(embeddings)):\n",
        "        for j in range(i + 1, len(embeddings)):\n",
        "            sim = sim_matrix[i, j]\n",
        "            if sim > threshold:\n",
        "                net.add_edge(i, j, value=float(sim), title=f\"Similarity: {sim:.2f}\")\n",
        "\n",
        "    net.set_options(\n",
        "        \"\"\"\n",
        "    {\"physics\": {\"enabled\": false}}\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    net.save_graph(outfile)\n",
        "    abs_path = os.path.abspath(outfile)\n",
        "    print(f\"Graph saved to {abs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage: building the similarity graph\n",
        "\n",
        "# 1. Flatten Impresso results\n",
        "a_flat, texts_flat, uris_flat = flatten_results(\n",
        "    article_embeddings, articles, matches_uids\n",
        ")\n",
        "\n",
        "# 2. Prepare combined data\n",
        "all_embeddings = np.vstack([part_embeddings, a_flat])\n",
        "all_texts = parts + texts_flat\n",
        "node_types = [\"input\"] * len(part_embeddings) + [\"result\"] * len(a_flat)\n",
        "full_uris = [None] * len(part_embeddings) + uris_flat\n",
        "\n",
        "# 3. Dimensionality reduction (UMAP)\n",
        "coords = reduce_embeddings(all_embeddings)\n",
        "\n",
        "# 4. Build and save graph (one call, no extra parameters needed)\n",
        "build_similarity_graph(\n",
        "    all_embeddings,\n",
        "    all_texts,\n",
        "    node_types,\n",
        "    full_uris,\n",
        "    coords,\n",
        "    threshold=0.7,\n",
        "    outfile=\"embedding_similarity_graph.html\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgL0Fj6lvDYO"
      },
      "source": [
        "## Bonus: Linking Images through Similarity\n",
        "\n",
        "The Impresso API also supports embeddings for images.  \n",
        "The process mirrors the text workflow:\n",
        "\n",
        "1. Provide an image (URL or local file).\n",
        "2. Generate an embedding via `tools.embed_image`.\n",
        "3. Retrieve visually related images from the Impresso collection.\n",
        "4. Build a similarity graph illustrating the connections.\n",
        "\n",
        "Image embeddings are based on a different underlying model than text embeddings.  \n",
        "They capture visual properties and may respond strongly to layout, contrast, or style.  \n",
        "You may replace the example image with any of your own files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gUDpSWjM7RM"
      },
      "source": [
        "#### Defining any image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J81guiV6MZ5w"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# picture of the notebook's author, feel free to exchange with any other image\n",
        "image_url = \"https://www.juriopitz.com/assets/img/me.png\"\n",
        "display(Image(url=image_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ZolnEzNDI9"
      },
      "source": [
        "#### Embedding this image and searching similar ones in Impresso\n",
        "\n",
        "Similar to how we embedded texts, we can also embed images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y18LKN-MT3k"
      },
      "outputs": [],
      "source": [
        "img_embedding = impresso_session.tools.embed_image(image=image_url, target=\"image\")\n",
        "matches = impresso_session.images.find(embedding=img_embedding, limit=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57l7n4KZNJ0G"
      },
      "source": [
        "#### Visualizing the result as a graph\n",
        "\n",
        "Similarly to how we created the connected network of input texts nd texts in impresso, we can create the connected network of the input image and images in impresso ---- All just based on semantic similarity of embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBZy5--k0YHx"
      },
      "outputs": [],
      "source": [
        "# --pre-processing-- for conveinence\n",
        "matches_uids = [datum[\"uid\"] for datum in matches.raw[\"data\"]]\n",
        "links_uids = [datum[\"contentItemUid\"] for datum in matches.raw[\"data\"]]\n",
        "matches_embeddings = [\n",
        "    impresso_session.images.get_embeddings(uid)[0] for uid in matches_uids\n",
        "]\n",
        "part_embeddings = [string2vector(emb) for emb in [embedding]]\n",
        "article_embeddings = [\n",
        "    [string2vector(emb) for emb in embs] for embs in [matches_embeddings]\n",
        "]\n",
        "matches_uids = [matches_uids]\n",
        "links_uids = [links_uids]\n",
        "\n",
        "flat_article_embeddings = np.vstack(article_embeddings)\n",
        "flat_articles = [r for sublist in articles for r in sublist]\n",
        "flat_uris = [None] * len(part_embeddings) + [\n",
        "    r for sublist in links_uids for r in sublist\n",
        "]\n",
        "\n",
        "# Combine everything for similarity computation\n",
        "all_embeddings = np.vstack([part_embeddings, flat_article_embeddings])\n",
        "all_texts = parts + flat_articles\n",
        "node_types = [\"input\"] * len(part_embeddings) + [\"result\"] * len(\n",
        "    flat_article_embeddings\n",
        ")\n",
        "\n",
        "# Dimensionality reduction (UMAP)\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "coords = reducer.fit_transform(all_embeddings)\n",
        "\n",
        "# Compute cosine similarities\n",
        "sim_matrix = cosine_similarity(all_embeddings)\n",
        "\n",
        "#  Build the PyVis network\n",
        "net = Network(height=\"700px\", width=\"100%\", notebook=True, cdn_resources=\"in_line\")\n",
        "scale = 150\n",
        "# Add nodes\n",
        "for i, (text, ntype) in enumerate(zip(all_texts, node_types)):\n",
        "    color = \"lightblue\" if ntype == \"input\" else \"lightgreen\"\n",
        "    size = 25 if ntype == \"input\" else 15\n",
        "    label = f\"I{i+1}\" if ntype == \"input\" else f\"R{i - len(flat_articles) + 1}\"\n",
        "    x, y = coords[i] * scale  # scale coordinates\n",
        "    url = (\n",
        "        None\n",
        "        if ntype == \"input\"\n",
        "        else \"https://dev.impresso-project.ch/app/article/\" + flat_uris[i]\n",
        "    )\n",
        "    title = (\n",
        "        \"Input Image\"\n",
        "        if ntype == \"input\"\n",
        "        else \"Impresso Image\" + f\"<br><a href={url} target='_blank'>Link</a>\"\n",
        "    )\n",
        "    net.add_node(\n",
        "        i,\n",
        "        label=label,\n",
        "        title=title,\n",
        "        color=color,\n",
        "        size=size,\n",
        "        x=float(x),\n",
        "        y=float(y),\n",
        "        fixed={\"x\": True, \"y\": True},  # properly fix both axes\n",
        "    )\n",
        "\n",
        "# Add edges based on similarity\n",
        "threshold = 0.0  # Adjust to make the graph denser/sparser\n",
        "for i in range(len(all_embeddings)):\n",
        "    for j in range(i + 1, len(all_embeddings)):\n",
        "        sim = float(sim_matrix[i, j])\n",
        "        if sim > threshold:\n",
        "            net.add_edge(i, j, value=sim, title=f\"Similarity: {sim:.2f}\")\n",
        "\n",
        "\n",
        "# --- Display inline in Colab ---\n",
        "net.set_options(\n",
        "    \"\"\"\n",
        "{\n",
        "  \"physics\": {\n",
        "    \"enabled\": false\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        ")\n",
        "net.save_graph(\"image_embedding_similarity_graph.html\")\n",
        "abs_path = os.path.abspath(\"image_embedding_similarity_graph.html\")\n",
        "display(HTML(\"image_ embedding_similarity_graph.html\"))\n",
        "print(f\"Graph saved to {abs_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUrMNOzOvUqk"
      },
      "source": [
        "## Using an External Embedding Model\n",
        "\n",
        "You can also load the same embedding model used in Impresso directly from the SentenceTransformers library.\n",
        "This allows you to compute embeddings offline, integrate them into your own workflows, or compare Impresso API outputs with embeddings generated locally.\n",
        "\n",
        "In this example, we load the `gte-multilingual-base` model, which underlies the Impresso text embedding system.\n",
        "We then compute an embedding for a simple test string and compare it with the corresponding embedding returned by the Impresso API.\n",
        "This verification step confirms that both sources produce consistent representations.\n",
        "\n",
        "Note that transformer models downloaded through SentenceTransformers are cached automatically within the current Colab runtime. Subsequent runs in the same runtime will therefore reuse the cached model without additional downloads, but a new Colab session starts with an empty cache and will trigger a fresh download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCo9qxDB5pUk"
      },
      "outputs": [],
      "source": [
        "%pip install 'sentence-transformers>=3.0.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqBqGpP2vlra"
      },
      "outputs": [],
      "source": [
        "# The first cell of revealing similarities and links can be replaced with this:\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name_or_path = \"Alibaba-NLP/gte-multilingual-base\"\n",
        "model = SentenceTransformer(model_name_or_path, trust_remote_code=True)\n",
        "part_embeddings = model.encode(parts, normalize_embeddings=True)\n",
        "article_embeddings = [\n",
        "    model.encode(articleset, normalize_embeddings=True) for articleset in articles\n",
        "]\n",
        "\n",
        "# verify that the embedding model works as intended:\n",
        "print(model.encode([\"hello\"], normalize_embeddings=True)[0][:4])\n",
        "print(string2vector(impresso_session.tools.embed_text(text=\"hello\", target=\"text\"))[:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook presented a complete workflow for linking external text and image collections to the Impresso archive using semantic embeddings.  \n",
        "You learned how to generate embeddings, run similarity searches, examine retrieved material, and explore connections through interactive visualisations.\n",
        "\n",
        "Embedding-based methods are effective for exploration, but their results should be interpreted with care.  \n",
        "Similarity scores reflect model behaviour and the quality of the underlying data, including OCR variation.  \n",
        "The visualisations are best viewed as an aid for discovery rather than as evidence of direct historical or semantic relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "You can continue exploring:\n",
        "\n",
        "- Notebook: Introduction to semantic similarity search\n",
        "- Notebook: Visualising newspaper collections\n",
        "- Notebook: Working with Impresso OCR and NLP tools\n",
        "\n",
        "Feel free to adapt this workflow to your own datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
