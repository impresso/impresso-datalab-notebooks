{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/blob/main/workshop_resources/ws4-embeddings/EventDetectionGranularity.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "Zpnt3sKOwFOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Granularity, Events and Embeddings\n",
        "\n",
        "This notebooks provides basic functionality for embedding texts at different levels, from the whole article to chunks and headlines. The main purpose is to showcase the retrieval and embedding functionalities of the Impresso API, as well as provide some code for visualising embeddings using dimensionality reduction."
      ],
      "metadata": {
        "id": "wuFIdTb6PFJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages"
      ],
      "metadata": {
        "id": "prywZC0qPxep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NA00tr6LzZN"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install pandas chonkie faiss-cpu tqdm seaborn plotly umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install git+https://github.com/impresso/impresso-py.git@embeddings-search"
      ],
      "metadata": {
        "id": "iPx0NFTcRJ4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the kernel just in case...\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "YVy2v_NNRPQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and process data"
      ],
      "metadata": {
        "id": "FVpoB1ENh432"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1H8_1-PbGPlcrm3wvwd1xGaUrhNnYETha"
      ],
      "metadata": {
        "id": "lu-_kqnhP3Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the data for the general query\n",
        "!unzip -o Olympics-general.zip -d data"
      ],
      "metadata": {
        "id": "mCUBefcqR37l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from chonkie import SemanticChunker\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "import numpy as np\n",
        "from impresso import connect"
      ],
      "metadata": {
        "id": "BJ1FwD3KSXTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to the Impresso client"
      ],
      "metadata": {
        "id": "OZW_pFJEQlOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "impresso_session = connect('https://dev.impresso-project.ch/public-api/v1')"
      ],
      "metadata": {
        "id": "bIX4ssNQ3omM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load helper functions"
      ],
      "metadata": {
        "id": "Z81cmabaQtRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embed text helper functions\n",
        "import time\n",
        "import base64\n",
        "import struct\n",
        "\n",
        "def embed_text(text: str, target: str):\n",
        "  \"\"\"\n",
        "  Convert text to embedding, return None in case of an error\n",
        "  \"\"\"\n",
        "  time.sleep(1)\n",
        "  try:\n",
        "    return impresso_session.tools.embed_text(text, target)\n",
        "  except Exception as e:\n",
        "    print(text)\n",
        "    print(e)\n",
        "    return None\n",
        "\n",
        "\n",
        "def convert_embedding(embedding: np.float32):\n",
        "  \"\"\"\n",
        "  Convert base64 string to a float array\n",
        "  \"\"\"\n",
        "  if not embedding:\n",
        "    return None\n",
        "\n",
        "  _, arr = embedding.split(':')\n",
        "  arr = base64.b64decode(arr)\n",
        "  outof_corpus_emb = [struct.unpack('f', arr[i:i+4])[0] for i in range(0, len(arr), 4)]\n",
        "  return outof_corpus_emb"
      ],
      "metadata": {
        "id": "6seDwRD0Dqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "\n",
        "The data file is csv document containing articles mentioning \"Olympic Games\""
      ],
      "metadata": {
        "id": "clECk59iQ8MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CONFIG ---\n",
        "CSV_PATH = \"data/2025-10-20T13-28-55-45260b95.csv\"         # Path to your CSV file\n"
      ],
      "metadata": {
        "id": "9bTtQmTDTOnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CSV_PATH, sep=';',skiprows=4)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "EnRz8qTjToNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['year'].value_counts().sort_index().plot(kind='bar')"
      ],
      "metadata": {
        "id": "UuQpNXxbBlG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "-jlj6lh5V3dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to reduce the data a bit let's focus on the 30s and 40?\n",
        "df_period = df[df.year.between(1930,1950) & ~(df['title'].isnull())]\n",
        "df_period.shape"
      ],
      "metadata": {
        "id": "DYg-p0L4CI4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed Headlines"
      ],
      "metadata": {
        "id": "sGTDdLCURHoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's first get the embeddings of the transcript title and look\n",
        "\n",
        "tqdm.pandas()\n",
        "df_period['title_embedding'] = df_period['title'].progress_apply(\n",
        "    lambda x: convert_embedding(embed_text(x,'text'))\n",
        "      )\n"
      ],
      "metadata": {
        "id": "pLChaL-gCXTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve transcript embeddings"
      ],
      "metadata": {
        "id": "jhzb8kQDRLic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the article embeddings from the API\n",
        "\n",
        "def get_embedding_by_uid(uid):\n",
        "  time.sleep(1)\n",
        "  try:\n",
        "    return convert_embedding(impresso_session.content_items.get_embeddings(uid)[0])\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(uid)\n",
        "    return None\n",
        "\n",
        "df_period['article_embedding']  = df_period.uid.progress_apply(get_embedding_by_uid)\n"
      ],
      "metadata": {
        "id": "mZRsGpzJ1nRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save data"
      ],
      "metadata": {
        "id": "s748HZOKRRRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_period.to_json('olympic-general-embedded.json')"
      ],
      "metadata": {
        "id": "A9UBCjGvs6Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Embeddings with UMAP\n",
        "\n",
        "Plot either transcript (i.e. article) or headline (i.e. title) embeddings with dimensionality reduction."
      ],
      "metadata": {
        "id": "0IT3LUHEiOsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 18SyEXcXjRTyu3UOzDOFZdDb16jA2ejmM"
      ],
      "metadata": {
        "id": "PX0t1G5NhnR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DIMENSIONALITY REDUCTION ---\n",
        "from umap import UMAP\n",
        "print(\"Reducing to 2D with UMAP...\")\n",
        "reducer = UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "EMBEDDING = 'article_embedding' # 'title_embedding' | 'article_embedding'\n",
        "\n",
        "df_period = pd.read_json('olympic-general-embedded.json')\n",
        "\n",
        "df_period = df_period[~df_period[EMBEDDING].isnull()]\n",
        "\n",
        "embeddings = list(df_period[EMBEDDING])\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "df_period[\"x\"] = embeddings_2d[:, 0]\n",
        "df_period[\"y\"] = embeddings_2d[:, 1]"
      ],
      "metadata": {
        "id": "HhltE1HQs8Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, max_len=250):\n",
        "    \"\"\"Truncate text and replace newlines for nicer tooltips\"\"\"\n",
        "    text = str(text).replace(\"\\n\", \" \")\n",
        "    return text[:max_len] + (\"...\" if len(text) > max_len else \"\")\n",
        "\n",
        "df_period[\"hover_text\"] = df_period.title.apply(clean_text)\n"
      ],
      "metadata": {
        "id": "LdQLgQC0gs7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalized, size = individual, colour = servants\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_period,\n",
        "                 x=\"x\",\n",
        "                 #size='all_inds',\n",
        "                 y=\"y\",\n",
        "                 #color=\"all_inds\",\n",
        "                 hover_data=['hover_text',\"year\"],\n",
        "                 width=1000, height=1000)\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "9qGNTIkvhJZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query\n",
        "\n",
        "The example code below shows how create a local vector database with FAISS, which you can then query. You should be able to create a database at different levels (i.e. article, title or chunk).\n",
        "\n",
        "The code for creating chunk level embeddings is shown below."
      ],
      "metadata": {
        "id": "Bov6dmHZjXNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Z5bGLddcCuwxAv4Ehu4Jt1QGNUh5CgYZ"
      ],
      "metadata": {
        "id": "cIXz83QRRuvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VECTOR STORE (FAISS) ---\n",
        "# save index\n",
        "\n",
        "EMBEDDING_LEVEL = 'chunk' # 'title' | 'article' | 'chunk'\n",
        "# see below for chunking script\n",
        "\n",
        "if EMBEDDING_LEVEL == 'chunk':\n",
        "  df_period = pd.read_json('olympic-general-chunks-sample-embedded.json')\n",
        "else:\n",
        "  df_period = pd.read_json('olympic-general-embedded.json')\n",
        "df_period = df_period[~df_period[f'{EMBEDDING_LEVEL}_embedding'].isnull()]\n",
        "\n",
        "embeddings = list(df_period[EMBEDDING])\n",
        "\n",
        "\n",
        "VECTOR_DB_PATH = f\"vector_db_{EMBEDDING_LEVEL}.faiss\"\n",
        "embeddings = np.array(list(df_period[f'{EMBEDDING_LEVEL}_embedding']), dtype=\"float32\")\n",
        "\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "faiss.write_index(index, VECTOR_DB_PATH)\n",
        "print(f\"Vector DB saved to {VECTOR_DB_PATH}\")\n"
      ],
      "metadata": {
        "id": "VrfGGvfZeqbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can search for different subthemes within the data (i.e. score each document to the similarity of the query embedding)"
      ],
      "metadata": {
        "id": "GdWcV1hXSSPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"Weltkrieg\"\n",
        "q_emb = convert_embedding(embed_text(query,'text'))\n",
        "\n",
        "D, I = index.search(np.array([q_emb], dtype=\"float32\"), k=5)\n",
        "print(\"Top 5 most similar chunks:\")\n",
        "print(df_period.iloc[I[0]])"
      ],
      "metadata": {
        "id": "OxeyZZaEkzF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk\n",
        "\n",
        "The code below shows how create chunk level embeddings. We apply this only to a small sample as it would take too long otherwise."
      ],
      "metadata": {
        "id": "EKX3hi-VI1My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHUNK TEXTS ---\n",
        "# Basic initialization with default parameters\n",
        "# see https://docs.chonkie.ai/oss/chunkers/semantic-chunker\n",
        "chunker = SemanticChunker(\n",
        "    embedding_model=\"minishlab/potion-base-32M\",  # Default model\n",
        "    threshold=0.8,                               # Similarity threshold (0-1)\n",
        "    chunk_size=256,                             # Maximum tokens per chunk\n",
        "    similarity_window=10,                         # Window for similarity calculation\n",
        "    skip_window=0                                # Skip-and-merge window (0=disabled)\n",
        ")\n"
      ],
      "metadata": {
        "id": "7loCzGKFQ3SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's go chunky!!\n",
        "\n",
        "chunks = []\n",
        "\n",
        "# COLUMN VARIABLES\n",
        "TEXT = 'transcript'\n",
        "\n",
        "print(\"Chunking text columns...\")\n",
        "for idx, row in tqdm(df_period.iterrows(), total=len(df_period)):\n",
        "        text = str(row[TEXT])\n",
        "        if text.strip():\n",
        "          chunks.append(chunker.chunk(text))\n",
        "\n",
        "df_period['chunks'] = chunks\n",
        "df_period_chunked = df_period.explode('chunks')\n",
        "df_period_chunked_sample = df_period_chunked.sample(1000, random_state=32)\n",
        "tqdm.pandas()\n",
        "df_period_chunked_sample['chunk_embedding'] = df_period_chunked_sample['chunks'].progress_apply(lambda x: convert_embedding(embed_text(x.text,'text')))\n",
        "\n"
      ],
      "metadata": {
        "id": "OlxJTGrQURsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_period_chunked_sample.reset_index(drop=True).to_json('olympic-general-chunks-sample-embedded.json')"
      ],
      "metadata": {
        "id": "gX_XvnBVdqLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fin"
      ],
      "metadata": {
        "id": "2wdkiS_SlOYX"
      }
    }
  ]
}
