{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L43GTakKxgO-"
   },
   "source": [
    "# News Agencies Recognition and Linking with Impresso BERT models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/blob/main/annotate/newsagency-processing_ImpressoHF.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "If something doesn't work, you can [report a problem](https://github.com/impresso/impresso-datalab-notebooks/blob/main/reporting-problems.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zTR3FjkxgPA"
   },
   "source": [
    "## What is this notebook about?\n",
    "\n",
    "This notebook demonstrates how to find mentions of news agencies in historical newspaper articles by loading a pre-trained model from Hugging Face. The model, part of the Impresso Project, is designed to recognize agency names such as Reuters, AFP, Havas, or DPA in texts written in French and German.\n",
    "\n",
    "It leverages a [language model](https://en.wikipedia.org/wiki/Language_model)-based (more exacly, [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))) multilingual named entity recognition ([NER](https://en.wikipedia.org/wiki/Named-entity_recognition)) model, fine-tuned on manually annotated articles from Swiss and Luxembourgish press (1840–2000), and provides a practical, reproducible way to detect these mentions in your own text collections. More details about how the model and data were created are detailed in the Master thesis of [Marxen, 2023](https://infoscience.epfl.ch/entities/publication/0ca6e53d-d37e-4cdf-a360-2dd9f34a8271).\n",
    "\n",
    "\n",
    "## Why is this useful?\n",
    "\n",
    "Delivering swift and reliable news since the 1830s and 1840s, news agencies have played a pivotal role both nationally and internationally. However, understanding their precise influence on shaping historical news content has remained somewhat elusive. Our goal is to illuminate this hidden infrastructure by identifying when and where agency material appeared in the press.\n",
    "\n",
    "We concentrated on articles we could confidently identify as having agency content, which motivated the following working definition:\n",
    "> *\"An article with news agency content is anything that explicitly cites a news agency as a source.\"*\n",
    "\n",
    "This approach ensures high-quality, unambiguous supervision during training — but it also means the model expects mentions to follow certain typical patterns.\n",
    "\n",
    "#### What counts as an “explicit” mention?\n",
    "\n",
    "In the training data, agency mentions most commonly appear in specific, identifiable formats, including:\n",
    "* At the beginning or end of an article:\n",
    "> ATS-REUTERS, Genève, mardi —\n",
    "> BERLIN, 10 mars (DPA) –\n",
    "* As a parenthetical or attributed clause:\n",
    "> La dépêche vient de (Reuter)\n",
    "> Selon l’A.F.P., le conflit s’aggrave…\n",
    "* As a stylistic line break after the dateline:\n",
    "> LONDRES, 2. — Agence — Les Anglais…\n",
    "\n",
    "Because these formats appeared more frequently in the dataset, the model is naturally better at detecting them. This is not due to any explicit bias in annotation, but rather a reflection of how agency content was typically presented in 19th–20th century newspaper writing.\n",
    "\n",
    "#### What the model might not detect\n",
    "\n",
    "Mentions embedded deep in the article body or framed narratively, like in the following example, are more difficult:\n",
    "\n",
    "> \"Reuters reports from Calcutta that the earthquake threatened part of eastern India...\"\n",
    "\n",
    "Even though this refers to a news agency (\"Reuters\"), the model might not recognise it as such because:\n",
    "* There is no explicit label or cue marking it as an agency (e.g., \"(EXTEL)\" or \"—Agence—\").\n",
    "* The mention is part of narrative reporting, not in a typographically distinct format.\n",
    "* The phrasing may not match surface forms seen frequently enough during training.\n",
    "\n",
    "#### Best practices for effective use\n",
    "\n",
    "To achieve the most reliable results:\n",
    "* Use texts where the agency name appears in a clear, structured format, ideally as part of the byline or first sentence.\n",
    "* Focus on articles where explicit agency attribution is given, especially through acronyms (e.g., AFP, SDA, DPA).\n",
    "* Be cautious with more literary or narrative texts, where attribution may be implicit or ambiguous.\n",
    "* When in doubt, consult [Marxen, 2023](https://infoscience.epfl.ch/entities/publication/0ca6e53d-d37e-4cdf-a360-2dd9f34a8271).\n",
    "\n",
    "\n",
    "If you're here, you likely seek to detect news agency entities in your own text. This notebook will walk you through how to load the model, apply it to your data, and interpret the output. If your material contains agency mentions in less-standardised forms, the detection of news agencies cannot be ensured.\n",
    "\n",
    "You can also access our [News Agency Recognition](https://huggingface.co/spaces/impresso-project/multilingual-news-agency-recognition) demo app through [HuggingFace Spaces](https://huggingface.co/docs/hub/en/spaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "This notebook uses a named entity recognition (NER) model built on top of dbmdz/bert-base-historic-multilingual-cased, fine-tuned specifically to identify news agency mentions in historical texts.\n",
    "\n",
    "The model is applied using the Hugging Face pipeline interface. It performs token classification, labeling each token in the input text with a BIO-style tag that indicates whether it belongs to a recognized agency (e.g., B-Reuters, I-Reuters) or not.\n",
    "\n",
    "Under the hood, the model has learned to associate known agency names, variants, and historical abbreviations (like \"Reuter\", \"A.F.P.\", \"dépêche Belga\", etc.) with specific tags. It also recognizes more generic cases such as ag (for \"agence\") and `articleauthor` (for personal bylines).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will you learn?\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "* Learn how to load and apply a custom NER model from Hugging Face using the transformers library\n",
    "* Understand how to identify mentions of news agencies in French and German historical newspaper articles\n",
    "* Explore how the model outputs structured information including entity type, surface form, position, and confidence\n",
    "* Gain insight into using entity recognition for media provenance analysis in historical datasets\n",
    "\n",
    "## Useful resources\n",
    "\n",
    "Here are some resources grouped by level of familiarity, with helpful reminders along the way:\n",
    "\n",
    "#### Beginner\n",
    "\n",
    "- [What is Hugging Face?](https://huggingface.co/learn/llm-course/chapter1/1)  \n",
    "  A simple introduction to the Hugging Face ecosystem and what it offers.\n",
    "\n",
    "- [Impresso Project Website](https://impresso-project.ch)  \n",
    "  Overview of the project behind this model, focusing on historical media analysis.\n",
    "\n",
    "> *\"If you don’t understand something now, don't worry — just keep going. It'll often make more sense later.\"* – [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)\n",
    "\n",
    "#### Intermediate\n",
    "\n",
    "- [Introduction to BERT Models](https://huggingface.co/transformers/model_doc/bert.html)  \n",
    "  A technical overview of how BERT works and how it’s used in NLP tasks.\n",
    "\n",
    "- [Named Entity Recognition with Transformers](https://huggingface.co/docs/transformers/main/en/task_summary#token-classification)  \n",
    "  Explanation of how to apply pretrained transformer models for NER tasks.\n",
    "\n",
    "> *\"Learning is not a linear process. Sometimes, understanding comes after you've seen the bigger picture.\"* – [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)\n",
    "\n",
    "#### Advanced\n",
    "\n",
    "- [Multilingual News Agency Recognition Demo (HF Spaces)](https://huggingface.co/spaces/impresso-project/multilingual-news-agency-recognition)  \n",
    "  Explore how the news agency recognition model performs in real-world examples, and try customizing inputs for evaluation.\n",
    "\n",
    "- [Model Source Code and Training Data](https://github.com/impresso/newsagency-classification)  \n",
    "  Dive into the codebase used for training and deploying the model, including data preparation and fine-tuning details.\n",
    "\n",
    "> *\"Trust yourself. You understand more than you think you do.\"* – [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVcCrAogxgPA"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install necessary libraries (if not already installed) and\n",
    "download the necessary NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install stopwordsiso\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfm_Fic-xgPC"
   },
   "source": [
    "## Basic Usage\n",
    "\n",
    "To get started, you can load the model directly from Hugging Face using the transformers pipeline. This gives you a simple interface to process texts and extract news agency mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUU_u7pBxgPF",
    "outputId": "e7531672-7f06-4ea6-8b8e-001e542dfc7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"newsagency-ner\", model=\"impresso-project/ner-newsagency-bert-multilingual\", trust_remote_code=True)\n",
    "\n",
    "text = \"REUTERS, Osaka – New severe earthquakes in Japan. Telephone and telegraph connections between Tokyo and Osaka have been disrupted by an earthquake. Trams in Tokyo are at a standstill. Fires have broken out in Sugamo, a Tokyo suburb. A train plunged into the Basubawa, a river between Gotemba and Tokyo. Six trains were overturned. In Yokohama, 600 houses were destroyed.\"\n",
    "results = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx7r6uOlxgPF"
   },
   "source": [
    "The output is a list of detected entities with their type, surface form, character offsets, and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Reuters',\n",
       "  'confidence': 0.95,\n",
       "  'index': 0,\n",
       "  'surface': 'REUTERS',\n",
       "  'start': 0,\n",
       "  'end': 7}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "In real-world use cases, especially when working with long articles or batches of OCRed texts, it can be useful to not just extract individual mentions of news agencies but to summarise what agencies appear, how confidently they were detected, and whether the article might be sourced from an agency.\n",
    "\n",
    "To help with this, we define a simple helper function `analyse_agency_mentions` that takes a full article, uses the model to extract agency mentions, and returns a short analysis including:\n",
    "* Which agencies were found\n",
    "* How often each one appears\n",
    "* The average confidence for each\n",
    "* A heuristic-based guess on whether the article might contain original news agency content\n",
    "\n",
    "This is particularly helpful if you're filtering large corpora to identify texts likely to have originated from agencies such as AFP, Havas, Reuters, or DPA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're working with longer articles or entire OCRed pages, it's often best to break the text into sentences or chunks before applying the model. This avoids truncation issues (BERT has a token limit of 512 tokens where [tokens are almost the equivalent of words](https://arxiv.org/pdf/2410.05864)) and improves precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'London, 3 September (AFP) – A powerful earthquake struck eastern Japan early this morning.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A long historical article (example)\n",
    "article = \"\"\"London, 3 September (AFP) – A powerful earthquake struck eastern Japan early this morning. \n",
    "Reuters reports that the tremors, which began at 3:57 a.m., caused widespread disruption in Tokyo and Yokohama. \n",
    "Fires broke out in the districts of Sugamo and Bunkyo, and several train lines have been suspended.\n",
    "\n",
    "According to (Reuters), over 600 homes have been destroyed, and dozens are missing or injured. \n",
    "AFP correspondents in Yokohama describe scenes of panic, with emergency services struggling to reach the worst-hit areas.\n",
    "\n",
    "Meanwhile, (DPA) confirms that the German consulate in Yokohama may have suffered structural collapse. \n",
    "A bulletin issued by DPA's Kobe bureau states that telegraph lines between Kobe and Tokyo are down, complicating relief efforts.\n",
    "\n",
    "Further updates are expected later today from (Reuters), (AFP), and (DPA).\n",
    "\"\"\"\n",
    "# Sentence splitting\n",
    "sentences = sent_tokenize(article, language=\"french\")\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyse_agency_mentions(sentences, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Analyse a list of sentences for news agency mentions and print the findings.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): The input sentences to analyze.\n",
    "        confidence_threshold (float): Minimum confidence to consider a detection valid.\n",
    "\n",
    "    Returns:\n",
    "        details (dict): Dictionary with agency stats and source-likeness scores.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        results += nlp(sentence)\n",
    "\n",
    "    if not results:\n",
    "        print(\"No news agency mentions found in the text.\")\n",
    "        return {}\n",
    "\n",
    "    # Collect agency hits\n",
    "    agency_hits = {}\n",
    "    for ent in results:\n",
    "        agency_type = ent.get(\"type\", \"unknown\")\n",
    "        confidence = ent.get(\"confidence\", 0.0)\n",
    "        agency_hits.setdefault(agency_type, []).append(confidence)\n",
    "\n",
    "    print(\"This article mentions the following news agencies:\")\n",
    "\n",
    "    total_mentions = sum(len(confidences) for confidences in agency_hits.values())\n",
    "    details = {}\n",
    "    source_scores = {}\n",
    "\n",
    "    for agency, confidences in agency_hits.items():\n",
    "        count = len(confidences)\n",
    "        avg_conf = round(np.mean(confidences), 2)\n",
    "        norm_count = count / total_mentions if total_mentions > 0 else 0\n",
    "        source_score = round(0.6 * norm_count + 0.4 * avg_conf, 2)\n",
    "\n",
    "        details[agency] = {\n",
    "            \"occurrences\": count,\n",
    "            \"avg_confidence\": avg_conf,\n",
    "            \"source_likeness_score\": source_score\n",
    "        }\n",
    "        source_scores[agency] = source_score\n",
    "        print(f\"- {agency}: {count} occurrence(s), avg. confidence {avg_conf:.2f}, source-likeness score: {source_score}\")\n",
    "\n",
    "    # Identify top candidate\n",
    "    top_agency = max(source_scores.items(), key=lambda x: x[1])[0]\n",
    "    top_score = source_scores[top_agency]\n",
    "\n",
    "    if top_score >= confidence_threshold:\n",
    "        print(f\"✅ This article might be original news agency material from {top_agency}.\")\n",
    "    else:\n",
    "        print(\"ℹ️ This article may reference agencies, but is unlikely to be original agency content.\")\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article mentions the following news agencies:\n",
      "- AFP: 3 occurrence(s), avg. confidence 0.99, source-likeness score: 0.6\n",
      "- Reuters: 3 occurrence(s), avg. confidence 0.82, source-likeness score: 0.53\n",
      "- DPA: 3 occurrence(s), avg. confidence 0.97, source-likeness score: 0.59\n",
      "✅ This article might be original news agency material from AFP.\n"
     ]
    }
   ],
   "source": [
    "details = analyse_agency_mentions(sentences, confidence_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFP': {'occurrences': 3, 'avg_confidence': 0.99, 'source_likeness_score': 0.6}, 'Reuters': {'occurrences': 3, 'avg_confidence': 0.82, 'source_likeness_score': 0.53}, 'DPA': {'occurrences': 3, 'avg_confidence': 0.97, 'source_likeness_score': 0.59}}\n"
     ]
    }
   ],
   "source": [
    "print(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function uses a confidence threshold (default 0.8) to decide whether an article is likely to contain primary agency content. This is based on the idea that original material typically begins with an explicit agency attribution (e.g., \"AFP, Berlin, 2 mars – ...\").\n",
    "\n",
    "You can adjust the threshold or modify the logic depending on how conservative or inclusive you want to be in defining \"first agency material.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided you with a practical introduction to identifying news agency mentions in historical newspaper articles using a multilingual NER model developed as part of the Impresso project. You learned how to load and apply the model, understand how it interprets explicit agency attributions, and even analyze whether an article might contain first-source agency content.\n",
    "\n",
    "When using this notebook, keep in mind that the model was trained on texts with explicit, surface-level mentions of agencies (e.g., AFP, DPA, Havas) and may not detect more subtle or implicit attributions. Its performance is best on OCR-cleaned historical articles in French or German, particularly from the 19th and 20th centuries.\n",
    "\n",
    "Despite these limitations, the tool offers a scalable, reusable method for investigating media sourcing, information flows, and content provenance in digitized historical corpora.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "That’s it for now! If you're interested in expanding your analysis, here are some next steps:\n",
    "\n",
    "- the [Visualising Place Entities on Maps](https://github.com/impresso/impresso-datalab-notebooks/blob/main/explore-vis/place-entities_map.ipynb) notebook, which demonstrates how to visualise in a map mentions to places in the Impresso corpus.\n",
    "- Apply this news agency pipeline to your own OCR collections, or combine it with metadata (dates, locations, publishers) to study the historical dynamics of international news coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuL2m3GR37rX"
   },
   "source": [
    "---\n",
    "## Project and License info\n",
    "\n",
    "### Notebook credits [![CreditLogo.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAARCAYAAAGnXcxvAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAhGVYSWZNTQAqAAAACAAFARIAAwAAAAEAAQAAARoABQAAAAEAAABKARsABQAAAAEAAABSASgAAwAAAAEAAgAAh2kABAAAAAEAAABaAAAAAAAAAJYAAAABAAAAlgAAAAEAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEqADAAQAAAABAAAAEQAAAACvX75vAAAACXBIWXMAABcSAAAXEgFnn9JSAAABWWlUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczp0aWZmPSJodHRwOi8vbnMuYWRvYmUuY29tL3RpZmYvMS4wLyI+CiAgICAgICAgIDx0aWZmOk9yaWVudGF0aW9uPjE8L3RpZmY6T3JpZW50YXRpb24+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgoZXuEHAAAD/UlEQVQ4ES1UW2hcVRRd+9w7907mkUfTmEczkzRpraH1gUWaZBKJ0BY/FKsSwS9BClbxv1V/RlAC/vghghShBVuVRo2GQv1Qa0xSalAi2sxMJNF2Yt7oJDOZmZs795zjvtMe5s7A2Xevvddaew/A51asz00CAun2Xp2bmNbpfb3aDyDFF/4vzR0eilBeHqc/AB15423/Ztii/NpzNNs5VB/c2M55pVn/VQTa+/4yy54t7JIDo/4otGQMIrMaTccSF1Oxfobhs9B8TN9CRKcfOa7nYJTE7vomJHZgn+yHB1lD89xJZXkNxN1RR3MFSe4zHU+Mp2N9Sz4EXcGw8XB87eWoVudz0iuR1DWwREW7SghDmD0rN8l8MLZ808vmHzW/GkH4l99C0aEEcqNfWw3Dp1C4MYM7l20tfDgGrH6s2D7kv59AeKAPheuTKCTPAU4FxFREpiPxmi3VBw50hTMC0FpxnhcSptWZnSZ+KSkISZWJJ15Q0EmtdZdB9Ilh1b1+YOGayzU0P8BiPKEdx63AMkBSmVpQuVEbof8s48We2y2jlIn1b6qGYNT7/QfbbD0GtfozVzrAqQsIdw+iGGyxhRUQe6k2ZLeOXUXo9NPY+9kXCAwPou2naRQX16XIr3wjdknr1pG3sPrsUzD21MOsjaI8egGluQzEQ+1Ca/m4sDXR6tl30Hb1W0ABW+PXfEGgHRdwpSISk35PORUyI2p+xiTrAVBzEDANYKkIq6UGOlgfrLL7Jz6g87Li0zUZmB1EJQwRKJJ4qSfbetlXnNqzUxyyXtFCbPiVBInxC0snjZ7s1CXCqPQVJ0aozuGf+xMnlNQjURJHLYYrKYmy3yhHQ1ygRhhwtEYResqDOHckOznt5zPw3a9UbPBUrcBYcdeFrLOLVBcKUZG1KTHBxrBPEnqjAIraYEmVIpRtpxKmnIPdgHii5/bUjyIV7z/boPVYPkiOXFuGml8MW48dIXGwHervAsyDcYhYE+TaDPNrhOhuE2a8OaS62+BsO049ieupjsHTlGrvK4uoFaykJ9D0+ZeAlNj6+FPI67PYc+k9qJ0deP/mUJvoxZ2hATSdvwir+T6sP3MG4lCnNssV8jRWqqNbFcjnabAVzFjOZlmWErTrIti1H7WD/dh4/yNeJLZ2hxXie40se8M390QiXs8360Dvbtlw9cKyhUgDrOd57rYKkL8uAGELen4TxtAhqMwKELHuJvNuKEFuA5nWNvSZquKZ+MBwHdGVnOcqTpREFIDkhVBczn/YKVj8v1B0QbwB7L2LXc+KGgF2FSfuz059V7X/nn060znwJAN8WEeii31Cmfeu4oPwCfCQ1JBRpVfQas5D4NXDSxOTHK2Oz/88172PBBmtMAAAAABJRU5ErkJggg==)](https://credit.niso.org/)\n",
    "**Writing - Original draft:** Emanuela Boros. **Conceptualization:** Emanuela Boros, Maud Ehrmann. **Software:** Lea Marxen, Emanuela Boros, Maud Ehrmann. **Writing - Review & Editing:** Caio Mello. **Validation:** Kirill Veprikov, Cao Vy. **Datalab editorial board:** Caio Mello (Managing), Pauline Conti, Emanuela Boros, Marten Düring, Juri Opitz, Martin Grandjean, Estelle Bunout, Cao Vy. **Data curation & Formal analysis:** Maud Ehrmann, Emanuela Boros, Pauline Conti, Simon Clematide, Juri Opitz, Andrianos Michail. **Methodology:** Emanuela Boros, Maud Ehrmann. **Supervision:** Emanuela Boros, Maud Ehrmann. **Funding aquisition:** Maud Ehrmann, Simon Clematide, Marten Düring, Raphaëlle Ruppen Coutaz.\n",
    "\n",
    "<br><a target=\"_blank\" href=\"https://creativecommons.org/licenses/by/4.0/\">\n",
    "  <img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by.png\"  width=\"100\" alt=\"Open In Colab\"/>\n",
    "</a> \n",
    "\n",
    "This notebook is published under [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0/)\n",
    "\n",
    "For feedback on this notebook, please send an email to info@impresso-project.ch\n",
    "\n",
    "### Impresso project\n",
    "\n",
    "[Impresso - Media Monitoring of the Past](https://impresso-project.ch) is an interdisciplinary research project that aims to develop and consolidate tools for processing and exploring large collections of media archives across modalities, time, languages and national borders. The first project (2017-2021) was funded by the Swiss National Science Foundation under grant No. [CRSII5_173719](http://p3.snf.ch/project-173719) and the second project (2023-2027) by the SNSF under grant No. [CRSII5_213585](https://data.snf.ch/grants/grant/213585) and the Luxembourg National Research Fund under grant No. 17498891.\n",
    "<br></br>\n",
    "### License\n",
    "\n",
    "All Impresso code is published open source under the [GNU Affero General Public License](https://github.com/impresso/impresso-pyindexation/blob/master/LICENSE) v3 or later.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/impresso/impresso.github.io/blob/master/assets/images/3x1--Yellow-Impresso-Black-on-White--transparent.png?raw=true\" width=\"350\" alt=\"Impresso Project Logo\"/>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
