{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFaBppP2Sb6z"
      },
      "source": [
        "# Solr Normalization with impresso-pipelines Package\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/blob/main/annotate/solrnormalization_pipeline_demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is this notebook about?\n",
        "\n",
        "This notebook introduces the [solrnormalization](https://github.com/impresso/impresso-pipelines/tree/main/impresso_pipelines/solrnormalization) component of the [impresso-pipelines](https://pypi.org/project/impresso-pipelines/) Python package.\n",
        "\n",
        "The goal of the `impresso-pipelines` python module is to **make the internal data processing workflows of the Impresso Web App transparent and reproducible**. `impresso-pipelines` offers low-code, ready-to-use pipelines that require minimal configuration and allow users —such as researchers, developers, or digital humanities practitioners— to apply the *same processing steps* we used on our historical newspaper collections to their *own* text collections.\n",
        "\n",
        "One of these processes is **text indexing**, which we do with the Solr search platform, to support information retrieval (keyword search and beyond) on the Impresso corpus. Before building the index, Solr applies a series of language analysis and normalization steps to the text. The same processes are applied to users' query terms.\n",
        "\n",
        "This notebook allows to **replicate Solr's normalization steps** by providing a standardized, language-aware tokenization and normalization pipeline that simulates the transformations applied during text indexing and query processing (To retrieve matching results, Solr must apply exactly the same normalization steps to both the indexed text and the user’s query).\n",
        "\n",
        "This notebook therefore helps **help understand how Solr text normalization works** by exposing the exact tokens produced after language-specific analyzers, filters, and stopword lists have been applied.\n",
        "\n",
        "**A word on Solr normalization steps**\n",
        "\n",
        "Typical language normalization steps applied by indexing systems include the following (for technical background, you may refer to the [Solr documentation](https://solr.apache.org/guide/solr/latest/indexing-guide/language-analysis.html)). We do not detail each step but present them in general terms:\n",
        "\n",
        "- Tokenisation: A process by which text sequences are broken into linguistic units (no sub-word tokenisation here). Several [tokenisers](https://solr.apache.org/guide/solr/latest/indexing-guide/tokenizers.html) are available in Solr; the Impresso indexing pipeline use the Standard Tokeniser.\n",
        "Lower-casing: search engines typically convert all text to lowercase before indexing.\n",
        "\n",
        "- Stopwords removal: (common words like \"the\", \"and\", \"is\") appear very frequently and are typically not searched for. As they would also need a lot of storage (e.g. in English, 1/6th of all tokens is the word \"the\"), it is also resource-efficient to remove them from the index.\n",
        "\n",
        "- Stemming: European languages found in our Impresso collection are inflecting, meaning that the same word can appear in many different forms (e.g., \"introduce\", \"introduces\", \"introduced\", \"introducing\"). These forms are reduced to a base form through *stemming*, that allows to find all forms of a word with a single search term.\n",
        "\n",
        "- Text collections—especially historical ones—often contain inconsistencies in spelling and linguistic conventions across time periods and sources, the use of special characters, diacritics, accents can vary over time. Ignoring diacritics and mapping non-ASCII characters to their ASCII equivalents (a process called ASCII folding) helps to ensure that searches are not affected by these variations.\n",
        "\n",
        "## What will you learn?\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "- Understand the functionality of the `solrnormalization` subpackage from the `impresso-pipelines` package.\n",
        "- Learn how to normalize a raw text using language-specific Solr analyzers.\n",
        "- Explore different use cases, including **basic and advanced usage** of the SolrNormalization pipeline.\n",
        "- Recognize some limitations of the pipeline, such as support for **only selected languages** and the need for JVM integration.\n",
        "\n",
        "By the end of this notebook, you will have a clear understanding of how Solr-style normalization is applied to text and how it can be used in practical scenarios.\n",
        "\n",
        "## Background: How it works\n",
        "For technical details on this library, please refer to the repository of the [impresso-pipelines](https://github.com/impresso/impresso-pipelines/tree/main) package.\n",
        "\n",
        "The SolrNormalization component reproduces the language-aware text processing pipeline used in Impresso Solr indexing, by combining Python and Lucene Java-based analyzers in a unified workflow. The process follows these steps:\n",
        "\n",
        "- Initialization: Load Lucene/Solr analyzers and stopword lists for all supported languages, each with its own tokenizer and normalization rules. A generic analyzer is also loaded for unsupported or unknown languages.\n",
        "(*Technical background: Solr analyzers—based on Apache Lucene—apply tokenization, lowercasing, stopword removal, ASCII folding, and stemming in a configurable sequence. We use Lucene’s CustomAnalyzer to reproduce Impresso’s exact configuration for each language.*)\n",
        "\n",
        "- Language Detection: Automatically detect the language of the input text unless a language is provided manually.\n",
        "(*Technical background: This step can use the *[LangIdentPipeline](https://github.com/impresso/impresso-pipelines/tree/main/impresso_pipelines/langident)* from `impresso-pipelines` to select the appropriate language-specific analyzer.*)\n",
        "\n",
        "- Analysis: Apply language-specific processing using Solr components, including tokenization, lowercasing, stopword removal, ASCII folding, and stemming, fully mirroring the transformations used in Impresso’s indexing workflows.\n",
        "(*Technical background: Through JPype, Python code directly accesses the Java-based Lucene/Solr analysis pipeline.*)\n",
        "\n",
        "- Output: Return the list of normalized tokens together with the detected (or specified) language.\n",
        "(*Technical background: The component supports cleanup of Java resources via a context manager or destructor.*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ek_DUzRS5wR"
      },
      "source": [
        "## Useful resources\n",
        "\n",
        "- For technical details on this library, please refer to the repository of the [Impresso Pipelines package](https://github.com/impresso/impresso-pipelines/tree/main).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBdL2WC0S-vh"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "First, start by installing the `impresso-pipelines` package. Please note that it might require you to restart the runtime to apply changes. To do so, on Google Colab, go to _Runtime_ and select _Restart session_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jc2KhsawQ8bv"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade \"impresso-pipelines[solrnormalization]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW6_QLg_THhc"
      },
      "source": [
        "## Basic Usage\n",
        "\n",
        "Import the normalization pipeline from the package and create an instance of the\n",
        "`SolrNormalization` class. This class provides methods to normalize text in multiple languages using Solr\n",
        "analyzers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1kYDZMOTHqI"
      },
      "outputs": [],
      "source": [
        "from impresso_pipelines.solrnormalization import SolrNormalizationPipeline\n",
        "\n",
        "# We name the instance of SolrNormalizationPipeline as solrnormalization_pipeline\n",
        "solrnormalization_pipeline = SolrNormalizationPipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zWWVmCcTqsl"
      },
      "source": [
        "Let's take an example paragraph in German, with special characters and different punctuation marks, and pass it in the `solrnormalisation` pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4yNrxseTMdl"
      },
      "outputs": [],
      "source": [
        "de_text = \"\"\"\n",
        "In den frühen Morgenstunden verließ der Geschäftsführer der Münchner Rückversicherungsgesellschaft das Gebäude – ein Ereignis,\n",
        "das unter den Mitarbeitenden für großes Aufsehen sorgte. Außerdem wurde über „öffentliche“ Statements spekuliert, obwohl keine\n",
        "offiziellen Informationen verfügbar waren.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_JPtOlIVQCU"
      },
      "outputs": [],
      "source": [
        "solrnormalization_pipeline(de_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLbfwYjMMOrD"
      },
      "source": [
        "**Interpretation of the result:**\n",
        "\n",
        "As can be seen, the returned result is a dictionary with two keys: `\"language\"` and `\"tokens\"`.\n",
        "\n",
        "- `language` indicates the language detected for the input text (in this case, `'de'` for German). If a language is manually specified, it will override the detection.\n",
        "\n",
        "- `tokens` is a list of normalized tokens extracted from the text using a Lucene analyzer. These tokens are lowercased, stripped of stopwords, and normalized to remove accents and standardize common linguistic variants.\n",
        "\n",
        "The output shows how long compound words (e.g., `Rückversicherungsgesellschaft`), special characters (e.g., `ü` in `früh`), and punctuation (e.g., quotes and dashes) are handled and broken down into analyzable components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kxpw368QbD6"
      },
      "source": [
        "## Advanced Usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COx0hlwmQmWT"
      },
      "source": [
        "This pipeline currently supports two optional attributes when calling it, allowing limited control over its behavior while keeping usage simple.\n",
        "\n",
        "- `lang`: expects a string language code (e.g., `'de'` or `'fr'`). If provided, the pipeline skips automatic language detection and directly applies the corresponding Lucene analyzer. This can improve performance slightly and is useful in controlled multilingual settings.\n",
        "\n",
        "- `diagnostics`: expects a Boolean value. If set to `True`, the pipeline will return additional information such as removed stopwords from the provided text.\n",
        "\n",
        "These attributes can be used individually, in combination with each other, or all at once, depending on the level of detail needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ManNU1dtSXbl"
      },
      "source": [
        "**Example 1:** `lang`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5h95lR-LFUw"
      },
      "outputs": [],
      "source": [
        "solrnormalization_pipeline(de_text, lang=\"fr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I17tyTgTSiI1"
      },
      "source": [
        "If you specify `lang='fr'`, the pipeline will skip automatic language detection and apply the French analyzer directly—even if the input text is in another language.  \n",
        "This can be useful in controlled setups where you already know the correct language, or when comparing how the same text is normalized under different analyzers.\n",
        "\n",
        "In this example, we manually forced the pipeline to use the French analyzer on a German text. As a result, many German stopwords remain in the output, and language-specific filters (like German normalization or minimal stemming) are not applied.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDY-g0mDNJIi"
      },
      "source": [
        "**Example 2**: `diagnostics`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soDoUptZNWpQ"
      },
      "outputs": [],
      "source": [
        "solrnormalization_pipeline(de_text, diagnostics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cByOAmZMZ9hE"
      },
      "source": [
        "If you set `diagnostics=True`, the pipeline output will additionally include:\n",
        "\n",
        "- `stopwords_detected`: a list of words from the original text that were identified as stopwords and removed from the final tokens.\n",
        "- `analyzer_pipeline`: a list describing the sequence of processing steps (tokenizer and token filters) applied to the text for the detected language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NFhSDjRVTfX"
      },
      "source": [
        "**Example 3**: All at once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaHJhQ8_VUpo"
      },
      "outputs": [],
      "source": [
        "solrnormalization_pipeline(de_text, lang=\"de\", diagnostics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_aZm8lCcuMo"
      },
      "source": [
        "You can combine multiple options in a single call, like specifying the language and enabling diagnostics. The pipeline is flexible and allows using any combination of supported parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLlZHIPlWnwE"
      },
      "source": [
        "**Example 4**: Tokenization of common OCR errors (`^`, `_`, `-`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59ktEz3hY_9P"
      },
      "outputs": [],
      "source": [
        "altered_de_text = \"\"\"\n",
        "\n",
        "In den frühen Morge^stunden verließ der Geschäftsführer der Münchner Rückversicherungsgesellschaft das Gebäude – ein Ereignis,\n",
        "das unter den Mitar_eitenden für großes Aufsehen sorgte. Außerdem wurde über „öffentliche“ Statements spekuliert, obwohl keine\n",
        "offiziellen Inform-tionen verfügbar waren.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUkAkdwzZThd"
      },
      "outputs": [],
      "source": [
        "solrnormalization_pipeline(altered_de_text, diagnostics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuXxeBAFY-E9"
      },
      "source": [
        "This example illustrates how common OCR-related artifacts such as `^`, `_`, and hyphens are handled during normalization and tokenization.\n",
        "These characters often appear in digitized historical documents due to misrecognition or formatting inconsistencies.\n",
        "\n",
        "As can be seen above, `^` and `-` force words to be split, resulting in two different tokenized words. `_` on the other hand is just treated as part of the word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Z9qGJWY8Nk"
      },
      "source": [
        "## Limitations of the Solr Normalization Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYX9q5SDY-oP"
      },
      "outputs": [],
      "source": [
        "unsupported_languages = \"\"\"Στις πρώτες πρωινές ώρες, ο διευθύνων σύμβουλος της αντασφαλιστικής εταιρείας του Μονάχου αποχώρησε από το κτήριο – ένα γεγονός που προκάλεσε μεγάλη αναστάτωση μεταξύ των εργαζομένων. Επιπλέον, υπήρξαν εικασίες σχετικά με «δημόσιες» δηλώσεις, παρόλο που δεν υπήρχε καμία επίσημη ενημέρωση.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BYnm_iXZjHM"
      },
      "outputs": [],
      "source": [
        "solrnormalization_pipeline(unsupported_languages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO5HLC54aSAS"
      },
      "source": [
        "Currently, the Solr Normalization Pipeline offers dedicated analyzer pipelines for German, French, English, Spanish, Italian, Dutch, and Portuguese. For any other detected language, the pipeline automatically uses a simplified \"general\" analyzer, which performs fewer processing steps and does not remove stop words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM9fXpu4bxRN"
      },
      "source": [
        "## Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9101-ciSb0yg"
      },
      "source": [
        "The `SolrNormalizationPipeline` provides a lightweight, end-to-end solution for applying Solr-style normalization to German, French, English, Spanish, Italian, Dutch and Portugese texts. It delivers:\n",
        "\n",
        "- **Impresso-mirrored and consistent preprocessing**: Language-specific analyzers ensure uniform tokenization and normalization aligned with Solr standards.\n",
        "- **Easy Integration**: One-line setup and inference with minimal configuration.\n",
        "- **Optional Language Control**: Users can rely on built-in language detection or manually specify the language.\n",
        "- **Transparent Output**: Returns clean token lists that reveal exactly how input text is normalized.\n",
        "- **Future Extensibility**: Designed to be extended with more languages, diagnostics, and custom analyzer configurations.\n",
        "\n",
        "Whether you're preparing data for indexing, building reproducible NLP pipelines, or analyzing German and French corpora, this pipeline simplifies language-aware normalization—without requiring deep Lucene or Java expertise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL4iHEtob3KD"
      },
      "source": [
        "## Next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXhuPAb541"
      },
      "source": [
        "To get a better understanding of how this pipeline works, please check out the [original repository](https://github.com/impresso/impresso-pipelines/tree/main/impresso_pipelines/solrnormalization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2C0NVdVb-sR"
      },
      "source": [
        "---\n",
        "## Project and License info\n",
        "\n",
        "### Notebook credits [![CreditLogo.png](data:image/png;base64,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)](https://credit.niso.org/)\n",
        "\n",
        "INSERT CREDITS HERE\n",
        "<br></br>\n",
        "This notebook is published under [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0/)\n",
        "<br><a target=\"_blank\" href=\"https://creativecommons.org/licenses/by/4.0/\">\n",
        "  <img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by.png\"  width=\"100\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<br></br>\n",
        "\n",
        "### Impresso project\n",
        "\n",
        "[Impresso - Media Monitoring of the Past](https://impresso-project.ch) is an interdisciplinary research project that aims to develop and consolidate tools for processing and exploring large collections of media archives across modalities, time, languages and national borders. The first project (2017-2021) was funded by the Swiss National Science Foundation under grant No. [CRSII5_173719](http://p3.snf.ch/project-173719) and the second project (2023-2027) by the SNSF under grant No. [CRSII5_213585](https://data.snf.ch/grants/grant/213585) and the Luxembourg National Research Fund under grant No. 17498891.\n",
        "<br></br>\n",
        "### License\n",
        "\n",
        "All Impresso code is published open source under the [GNU Affero General Public License](https://github.com/impresso/impresso-pyindexation/blob/master/LICENSE) v3 or later.\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/impresso/impresso.github.io/blob/master/assets/images/3x1--Yellow-Impresso-Black-on-White--transparent.png?raw=true\" width=\"350\" alt=\"Impresso Project Logo\"/>\n",
        "</p>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
