{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge_Cq0FFaIrR"
   },
   "source": [
    "# Exploring Entity Co-occurrence Networks\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/4-impresso-py/network_graph.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this notebook about?\n",
    "\n",
    "This notebook guides you step-by-step through how to create network graphs that represent people's connections in historical newspapers. We define people's connections based on whether they occur in the same content item.\n",
    "\n",
    "> In Impresso, a **Content Item** is the smallest unit of editorial content within a newspaper or radio collection. This can be an article (for newspapers) or a radio show or episode (for radio programs). Content items can also vary by type, including articles, advertisements, tables, images, and more. Please note that when a newspaper does not have segmentation (OLR) content items for this title correspond to pages.\n",
    "\n",
    "With this notebook, you can produce a representation of the media narratives by looking at how people have been associated to others by the press. Interpreting this association, however, can be tricky. It doesn't mean necessarily that those people have had any type of relationship. It just means that their names have been mentioned in, for example, the same news article. To unveil the reasons why they occur together, further analysis using different methods is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will you learn?\n",
    "\n",
    "* How to retrieve a list of persons mentioned in content items for a given query; \n",
    "* To transform this list of entities into a dataframe that can be used to produce network graphs;\n",
    "* To display an interactive network graph to visualise connections between persons occurring in Impresso.\n",
    "\n",
    "You will be able to download the dataframes you produced, which guarantees reproducibility of your research. \n",
    "\n",
    "At the end, you can also download the network graph in different formats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful resources\n",
    "\n",
    "These are some resources that might help you with some important concepts this notebook involves:\n",
    "\n",
    "- [Exploring and Analyzing Network Data with Python](https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python)\n",
    "- [From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources](https://programminghistorian.org/en/lessons/creating-network-diagrams-from-historical-sources)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwR9UmpvaLhp"
   },
   "source": [
    "Install dependencies. You may need to restart the kernel to use updated packages. To do so, on Google Colab, go to *Runtime* and select *Restart session*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRpCUxZdaGUf"
   },
   "outputs": [],
   "source": [
    "%pip install -q impresso ipysigma networkx tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1NQ9Kq9aVm4"
   },
   "source": [
    "Connect to Impresso. The following command will prompt you to enter your Impresso token if it has not been authenticated recently (it expires after 8 hours).\n",
    "\n",
    "To get access to an Impresso API token, go to [Impresso Datalab](https://impresso-project.ch/datalab/) and select *Get API Token* on the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neqz3bBJaYr2"
   },
   "outputs": [],
   "source": [
    "from impresso import connect, OR, AND\n",
    "\n",
    "impresso_session = connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Prepare your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-2vEEpkawoj"
   },
   "source": [
    "## Get entities and their co-occurrences\n",
    "\n",
    "First, we retrieve all person entities mentioned in all articles that talk about the [Prague Spring](https://en.wikipedia.org/wiki/Prague_Spring) using search facets method from Impresso Python library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebAC0LuZEooc"
   },
   "outputs": [],
   "source": [
    "query = OR(\"Prague Spring\", \"Prager FrÃ¼hling\", \"Printemps de Prague\")\n",
    "\n",
    "persons = impresso_session.search.facet(\n",
    "  facet=\"person\",\n",
    "  term=query,\n",
    "  order_by=\"-count\",\n",
    "  limit=100\n",
    ")\n",
    "persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zJPhvgXcR9d"
   },
   "source": [
    "Next, we generate all unique pairs of entities with a mention count higher than `n`. This will filter out pairs of entities that are mentioned just a few times. \n",
    "\n",
    "**Important note:** for now, we are just combining all the entities in pairs. The documents in which they occur will be found later.\n",
    " \n",
    "First, entities that meet the mention threshold are selected, and then all possible pairs are generated using the `itertools.combinations` function.\n",
    "\n",
    "The `n` value can be adjusted so that we don't get too many entity combinations. A sweet spot is just under 500 combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noFYDdkTcbZU"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "n = 6\n",
    "\n",
    "df = persons.df\n",
    "df = df[df[\"count\"] > n]\n",
    "persons_ids = df.index.tolist()\n",
    "print(f\"Total persons selected: {len(persons_ids)}\")\n",
    "\n",
    "person_ids_combinations = list(itertools.combinations(persons_ids, 2))\n",
    "print(f\"Total combinations: {len(person_ids_combinations)}\")\n",
    "\n",
    "# The code below outputs an Execption message in case the number of combinations exceed 500. \n",
    "# If this happens to you, try to increase the value of 'n'. \n",
    "\n",
    "if len(person_ids_combinations) > 500:\n",
    "  msg = (\n",
    "      f\"The number of combinations is quite high ({len(person_ids_combinations)}). \" +\n",
    "      \"This may put a lot of load on Impresso and your requests may be throttled. \" +\n",
    "      \"Try to increase the threshold number of mentions in the cell above which will reduce the number of selected persons. \" +\n",
    "      \"You can also disable this error by commenting out this cell, if this number of combinations is expected.\"\n",
    "  )\n",
    "  raise Exception(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2tpzlOWdpjX"
   },
   "source": [
    "\n",
    "## Find articles where the entity pairs occur\n",
    "\n",
    "We also retrieve the dates and the number of articles where person entity pairs occur.\n",
    "\n",
    "This piece of code gets a facet for every combination of named entities. It is a single call per combination so it may take a while for a large number of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-yxLSCQdpAz",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from impresso.util.error import ImpressoError\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "connections = []\n",
    "\n",
    "# iterate over entity combinations, and build a query from each pair, faceting on `daterange`\n",
    "# the `query` variable hold the same value as above, i.e. keyword search for articles\n",
    "for idx, combo in tqdm(enumerate(person_ids_combinations), total=len(person_ids_combinations)):\n",
    "  try:\n",
    "    result = impresso_session.search.facet(\n",
    "      facet=\"daterange\",\n",
    "      term=query,\n",
    "      entity_id=AND(*combo),\n",
    "      limit=1000\n",
    "    )\n",
    "  except ImpressoError as e:\n",
    "    # a 429 status code means that the request has been throttled\n",
    "    # we sleep for 2 seconds and try again\n",
    "    if e.error.status == 429:\n",
    "      print(f\"Sleeping because of {e}\")\n",
    "      sleep(2)\n",
    "\n",
    "  if result.size > 0:\n",
    "    df = result.df\n",
    "\n",
    "    items = list(zip(df.index.tolist(), df['count'].tolist(), [result.url for i in range(len(df))]))\n",
    "    connections.append((combo, items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak1rEp7Omi6_"
   },
   "source": [
    "We put all in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkgCRrr3ez3r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "connections_denormalised = []\n",
    "for c in connections:\n",
    "  nodes, edges = c\n",
    "\n",
    "  connections_denormalised.extend(\n",
    "    [[node_a, node_b, ts, count, url] for (node_a, node_b), (ts, count, url) in zip([nodes for i in range(len(edges))], edges)]\n",
    "  )\n",
    "\n",
    "connections_df = pd.DataFrame(connections_denormalised, columns=('node_a', 'node_b', 'timestamp', 'count', 'url'))\n",
    "connections_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the connections into a CSV file that can be visualised independently in Part 2. Here you will be prompted to provide a name for the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "\n",
    "temp_dir = gettempdir()\n",
    "\n",
    "connections_csv_filename = input(\"Enter the filename: \").replace(\" \", \"_\")\n",
    "connections_csv_filepath = f\"{temp_dir}/{connections_csv_filename}.csv\"\n",
    "connections_df.to_csv(connections_csv_filepath)\n",
    "print(f\"File saved in {connections_csv_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDSucJXvoNTG"
   },
   "source": [
    "## Part 2: Visualise your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the CSV file you created in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juaVZgkWoRKL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "connections_df = pd.read_csv(connections_csv_filepath)\n",
    "connections_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we group results by frequency of pairs to create connections, and count the number of connections. We also preserve the URL. \n",
    "\n",
    "**Important information**: The URL does not contain DateRange information, that's why they can be grouped here as they just refer to the search terms and the pair of persons occuring in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oD3RMWtfmXw"
   },
   "outputs": [],
   "source": [
    "grouped_connections_df = connections_df.groupby(['node_a', 'node_b']) \\\n",
    "    .agg({'timestamp': lambda x: ', '.join(list(x)), 'count': 'sum', 'url': lambda x: list(set(x))[0]}) \\\n",
    "    .reset_index()\n",
    "grouped_connections_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we use the [NetworkX](https://networkx.org) python library, designed for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. \n",
    "\n",
    "Here, we start creating our network by defining the 'source' and 'taget', as well as the edges attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDvmvxTIo1-D"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    grouped_connections_df,\n",
    "    source='node_a',\n",
    "    target='node_b',\n",
    "    edge_attr=['count', 'url'],\n",
    "    create_using=nx.MultiGraph()\n",
    ")\n",
    "for i in sorted(G.nodes()):\n",
    "    G.nodes[i]['url'] = f\"https://impresso-project.ch/app/entities/{i}\"\n",
    "G.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reprocibility, save the file so that it can be downloaded and used elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlKjTDFqEAJ-"
   },
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "\n",
    "temp_dir = gettempdir()\n",
    "\n",
    "gefx_filename = input(\"Enter the gefx filename: \").replace(\" \", \"_\")\n",
    "gefx_filepath = f\"{temp_dir}/{gefx_filename}.gefx\"\n",
    "\n",
    "nx.write_gexf(G, gefx_filepath)\n",
    "\n",
    "print(f\"File saved in {gefx_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in Colab - activate custom widgets to allow `ipysigma` to render the graph. \n",
    "\n",
    "Ipysigma allows you produce an interactive graph, as well as manipulate the graph's settings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_98XMUAfU45K"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to render the graph. \n",
    "\n",
    "The output will prompt you to choose 'what should represent the size of the nodes' in your graph. Select it before you continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "node_size_widget = ipywidgets.Dropdown(\n",
    "    options=['Degree', 'Betweenness', 'Eigenvector', 'Closeness'],\n",
    "    value='Degree',\n",
    "    disabled=False,\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "ipywidgets.Box(\n",
    "    [\n",
    "        ipywidgets.Label(value='What should represent the size of the nodes:'), \n",
    "        node_size_widget\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh the next cell after changing the value above. \n",
    "\n",
    "This cell reads the node size method chosen above and plots the visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzXtYjKZUvZm"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from ipysigma import Sigma\n",
    "\n",
    "# Importing a gexf graph\n",
    "g = nx.read_gexf(gefx_filepath)\n",
    "\n",
    "node_size = None\n",
    "# Read node size method\n",
    "match node_size_widget.value:\n",
    "    case 'Degree':\n",
    "        node_size = g.degree\n",
    "    case 'Betweenness':\n",
    "        node_size = nx.betweenness_centrality(g)\n",
    "    case 'Eigenvector':\n",
    "        node_size = nx.eigenvector_centrality(g)\n",
    "    case 'Closeness':\n",
    "        node_size = nx.closeness_centrality(g)\n",
    "    case _:\n",
    "        node_size = g.degree\n",
    "\n",
    "print(f\"Node size method: {node_size_widget.value}.\")\n",
    "print(\"See the following link for more information about centrality measures: https://networkx.org/documentation/stable/reference/algorithms/centrality.html\")\n",
    "\n",
    "# Displaying the graph with a size mapped on degree and\n",
    "# a color mapped on a categorical attribute of the nodes\n",
    "Sigma(g, node_size=node_size, edge_size='count', clickable_edges=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph display allows you to download the visualisation in png, svg, gexf, and json. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided you with a comprehensive pipeline to create network graphs using the Impresso corpus. \n",
    "\n",
    "It is important to have in mind that only persons who have been tagged as entity 'persons' in the Impresso corpus will be added to this graph. Because of the way Named Entity Recognition (NER) works, it is possible that some people that are mentioned in the texts are not recognised as 'person' by the algorithms. In this case, those people will not be shown in the graph. For more information on NER, check our [FAQ](https://impresso-project.ch/app/faq#what-is-nep).\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "That's it for now! Next, you can explore:\n",
    "\n",
    "- the [Visualising Place Entities on Maps](https://github.com/impresso/impresso-datalab-notebooks/blob/main/annotate/NE-processing_ImpressoAPI.ipynb) notebook, which demonstrates how to visualise in a map mentions to places in the Impresso corpus.\n",
    "- the [Named Entity Recognition with impresso-pipelines]() notebook, which allows you reuse Impresso NER models and apply them to your own data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Project and License info\n",
    "\n",
    "### Credits [![CreditLogo.png](data:image/png;base64,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)](https://credit.niso.org/)\n",
    "\n",
    "\n",
    "### Impresso project\n",
    "\n",
    "[Impresso - Media Monitoring of the Past](https://impresso-project.ch) is an interdisciplinary research project that aims to develop and consolidate tools for processing and exploring large collections of media archives across modalities, time, languages and national borders. The first project (2017-2021) was funded by the Swiss National Science Foundation under grant No. [CRSII5_173719](http://p3.snf.ch/project-173719) and the second project (2023-2027) by the SNSF under grant No. [CRSII5_213585](https://data.snf.ch/grants/grant/213585) and the Luxembourg National Research Fund under grant No. 17498891.\n",
    "\n",
    "### Copyright\n",
    "\n",
    "Copyright (C) 2024 The Impresso team.\n",
    "\n",
    "### License\n",
    "\n",
    "This program is provided as open source under the [GNU Affero General Public License](https://github.com/impresso/impresso-pyindexation/blob/master/LICENSE) v3 or later.\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/impresso/impresso.github.io/blob/master/assets/images/3x1--Yellow-Impresso-Black-on-White--transparent.png?raw=true\" width=\"350\" alt=\"Impresso Project Logo\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TwR9UmpvaLhp",
    "q1NQ9Kq9aVm4"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
