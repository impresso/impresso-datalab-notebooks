{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8VMzOmOMJ5u"
   },
   "source": [
    "# Stratified Sampling of Articles into an Impresso User Collection\n",
    "\n",
    "## What is this notebook about?\n",
    "\n",
    "This notebook demonstrates how to perform stratified sampling on search results from the Impresso API. It allows you to systematically sample articles by year and newspaper, and add them to a collection for further analysis.\n",
    "\n",
    "## Why is this useful?\n",
    "\n",
    "When working with large historical newspaper archives, it is often impractical to analyze all available articles. Stratified sampling allows you to create a diverse subset of the data, ensuring that different time periods and publications are represented.\n",
    "\n",
    "## What will you learn?\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "- Connect to the Impresso API.\n",
    "- Perform faceted searches to get article counts per year and newspaper.\n",
    "- Implement a stratified sampling strategy to select articles.\n",
    "- Create a new Impresso user collection via the Impresso API.\n",
    "- Add sampled articles to an Impresso user collection.\n",
    "- Double-check the collection to ensure it contains the expected articles.\n",
    "\n",
    "## Recommended resources\n",
    "\n",
    "- [Impresso Python Documentation](https://pypi.org/project/impresso/)\n",
    "- [Video Tutorial on Impresso User Collections](https://vimeo.com/347022422)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bOYby77MCec"
   },
   "outputs": [],
   "source": [
    "%pip install -q impresso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a working client connection to the Impresso API\n",
    "\n",
    "Requires a valid API key and secret. You can obtain these from the Impresso website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb8RPt4LQsDQ"
   },
   "outputs": [],
   "source": [
    "from impresso import connect\n",
    "\n",
    "client = connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a proper logging that will be used throughout the notebook\n",
    "\n",
    "In order to better understand how the collection was created, we store rich information\n",
    "about the sampling process in a log file. This will help us trace the steps taken during the sampling and collection creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_logging(log_filename: str = \"sampling_log.txt\"):\n",
    "    \"\"\"\n",
    "    Set up logging to a user-specified file.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file. Defaults to \"sampling_log.txt\".\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear existing handlers\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_filename, mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # Prevent duplicate logs\n",
    "    logger.propagate = False\n",
    "\n",
    "    # Get absolute path of the log file\n",
    "    absolute_log_path = os.path.abspath(log_filename)\n",
    "    print(f\"Logging configured. Logs will be saved to: {absolute_log_path}\")\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Set up default logging\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure logging (optional)\n",
    "\n",
    "You can specify a custom log file to trace the sampling process. If you don't run this\n",
    "cell, logging will use the default output file \"sampling_log.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to use a custom log file\n",
    "# logger = setup_logging(\"my_custom_sampling_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-UVW3lSPicv"
   },
   "source": [
    "## Sampling function that retrieves article IDs in ascending time order\n",
    "\n",
    "Let's define a function that retrieves article IDs in ascending time order, which is useful for sampling articles from the Impresso API.\n",
    "In order to be gentle on the API, we will add a delay between requests. This is\n",
    "important to avoid overwhelming the API with too many requests in a short period of\n",
    "time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRIBVNNyPggt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from impresso import DateRange\n",
    "from impresso.client import ImpressoClient\n",
    "\n",
    "\n",
    "def sample_impresso_uids(\n",
    "    client: ImpressoClient,\n",
    "    keyword: str,\n",
    "    start_date: str | None = None,\n",
    "    end_date: str | None = None,\n",
    "    limit_per_query: int = 20,\n",
    "    max_hits: int = 20,\n",
    "    delay: float = 1.0,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Sample article UIDs from Impresso API based on a search keyword and date range.\n",
    "\n",
    "    Args:\n",
    "        client (connect): Impresso API client.\n",
    "        keyword (str): Keyword to search for.\n",
    "        start_date (str | None): Start date for filtering (YYYY-MM-DD format).\n",
    "        end_date (str | None): End date for filtering (YYYY-MM-DD format).\n",
    "        limit_per_query (int): Maximum number of articles per query.\n",
    "        max_hits (int): Maximum number of articles to sample.\n",
    "        delay (float): Delay in seconds between API requests.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of sampled article UIDs.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If limit_per_query is not between 1 and 100.\n",
    "        Exception: If API requests fail.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logger.info(f\"Starting sampling process for keyword: '{keyword}'\")\n",
    "    logger.debug(\n",
    "        f\"Parameters: limit_per_query={limit_per_query}, max_hits={max_hits},\"\n",
    "        f\" delay={delay}\"\n",
    "    )\n",
    "\n",
    "    if not 0 < limit_per_query <= 100:\n",
    "        logger.error(\n",
    "            f\"Invalid limit_per_query: {limit_per_query}. Must be between 1 and 100.\"\n",
    "        )\n",
    "        raise ValueError(\n",
    "            f\"Invalid limit_per_query: {limit_per_query}. Must be between 1 and 100.\"\n",
    "        )\n",
    "\n",
    "    sampled_uids = []\n",
    "    found = 0\n",
    "\n",
    "    # Step 1: Get all years with mentions of the keyword in the date range\n",
    "    logger.debug(\"Step 1: Fetching year facets for keyword\")\n",
    "    if start_date or end_date:\n",
    "        date_range = DateRange(start_date, end_date)\n",
    "        logger.info(f\"Using date range: {date_range}\")\n",
    "    else:\n",
    "        date_range = None\n",
    "        logger.info(\"No date range specified, using all available data.\")\n",
    "\n",
    "    try:\n",
    "        year_hits = client.search.facet(\n",
    "            \"year\", term=keyword, date_range=date_range, limit=200\n",
    "        ).raw\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch year facets: {e}\")\n",
    "        raise\n",
    "\n",
    "    year_buckets = year_hits.get(\"data\", [])\n",
    "    logger.debug(f\"Year facets: {year_hits}\")\n",
    "\n",
    "    if not year_buckets:\n",
    "        logger.warning(f\"No hits found for keyword: '{keyword}'\")\n",
    "        return []\n",
    "\n",
    "    sorted_year_buckets = sorted(year_buckets, key=lambda b: b.get(\"value\"))\n",
    "    logger.info(f\"Found {len(sorted_year_buckets)} years mentioning '{keyword}'\")\n",
    "    logger.info(f\"Years found: {[b.get('value') for b in sorted_year_buckets]}\")\n",
    "\n",
    "    for year_bucket in sorted_year_buckets:\n",
    "        year = year_bucket.get(\"value\")\n",
    "        if not year:\n",
    "            continue\n",
    "\n",
    "        logger.debug(f\"Processing year: {year}\")\n",
    "        date_range = DateRange(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "\n",
    "        # Step 2: For each year, get all newspapers with hits\n",
    "        logger.info(f\"Step 2: Fetching newspaper facets for year {year}\")\n",
    "        newspapers_raw = client.search.facet(\n",
    "            \"newspaper\", term=keyword, date_range=date_range, limit=200\n",
    "        ).raw\n",
    "        newspaper_buckets = newspapers_raw.get(\"data\", [])\n",
    "        logger.info(f\"Newspaper facets for {year}: {newspapers_raw}\")\n",
    "\n",
    "        if not newspaper_buckets:\n",
    "            logger.warning(f\"No newspapers found for year {year}\")\n",
    "            continue\n",
    "\n",
    "        logger.debug(f\"Found {len(newspaper_buckets)} newspapers for year {year}\")\n",
    "\n",
    "        for paper in newspaper_buckets:\n",
    "            newspaper_id = paper.get(\"value\")\n",
    "            if not newspaper_id:\n",
    "                logger.warning(f\"Missing newspaper ID in facet bucket: {paper}\")\n",
    "                continue\n",
    "\n",
    "            logger.debug(f\"Processing newspaper: {newspaper_id} for year {year}\")\n",
    "\n",
    "            try:\n",
    "                logger.debug(\n",
    "                    f\"Searching for articles in {newspaper_id} for year {year}\"\n",
    "                )\n",
    "                results = client.search.find(\n",
    "                    term=keyword,\n",
    "                    newspaper_id=newspaper_id,\n",
    "                    date_range=date_range,\n",
    "                    with_text_contents=False,\n",
    "                    limit=limit_per_query,\n",
    "                ).raw\n",
    "                hits = results.get(\"data\", [])\n",
    "                logger.debug(\n",
    "                    f\"Found {len(hits)} hits for '{newspaper_id}' in {year}. Waiting\"\n",
    "                    f\" for {delay} seconds...\"\n",
    "                )\n",
    "                time.sleep(delay)  # Respectful delay between requests\n",
    "\n",
    "                if hits:\n",
    "                    hit = random.choice(hits)\n",
    "                    uid = hit.get(\"uid\")\n",
    "                    if uid:\n",
    "                        logger.debug(\n",
    "                            f\"Selected UID: {uid} from {newspaper_id} in {year}\"\n",
    "                        )\n",
    "                        sampled_uids.append(uid)\n",
    "                        found += 1\n",
    "                        logger.info(\n",
    "                            f\"Progress: {found}/(max.){max_hits} articles sampled\"\n",
    "                        )\n",
    "                        if found >= max_hits:\n",
    "                            logger.info(\n",
    "                                f\"Reached maximum number of articles ({max_hits})\"\n",
    "                            )\n",
    "                            return sampled_uids\n",
    "                else:\n",
    "                    logger.debug(f\"No results for {newspaper_id} in {year}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing '{newspaper_id}' in {year}: {e}\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Sampling completed. Collected {len(sampled_uids)} UIDs for keyword\"\n",
    "        f\" '{keyword}'\"\n",
    "    )\n",
    "    return sampled_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llGn7PS9Q4sF"
   },
   "outputs": [],
   "source": [
    "doc_ids = sample_impresso_uids(client, \"United Press International\", max_hits=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to create and populate an Impresso user collection\n",
    "\n",
    "We will define functions to create a new Impresso user collection and add sampled articles to it.\n",
    "If the collection already exists, we will skip the creation step and proceed to add\n",
    "articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8GxQVg5RfsU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def create_collection(client, name: str, description: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Create a new collection in the Impresso public API using a client with a stored bearer token.\n",
    "\n",
    "    Args:\n",
    "        client: An authenticated Impresso API client with a `_api_bearer_token` attribute.\n",
    "        name (str): Name of the collection.\n",
    "        description (str): Optional description of the collection.\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response from the API.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the client does not have a valid _api_bearer_token.\n",
    "        RuntimeError: If the API request fails.\n",
    "    \"\"\"\n",
    "    token = getattr(client, \"_api_bearer_token\", None)\n",
    "    if not token:\n",
    "        raise ValueError(\"Client does not have a valid _api_bearer_token.\")\n",
    "\n",
    "    url = \"https://impresso-project.ch/public-api/v1/collections\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    payload = {\"name\": name, \"description\": description, \"accessLevel\": \"private\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.ok:\n",
    "        logging.info(f\"Collection '{name}' created successfully.\")\n",
    "        logging.debug(f\"Response: {response.json()}\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to create collection: {response.status_code} {response.text}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def add_docids(\n",
    "    client: connect,\n",
    "    collection_id: str,\n",
    "    doc_ids: list[str],\n",
    "    delay: float = 30,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Add article IDs to a collection in batches of 200.\n",
    "\n",
    "    Args:\n",
    "        client (connect): Impresso API client.\n",
    "        collection_id (str): ID of the user collection.\n",
    "        doc_ids (list[str]): List of article IDs to add.\n",
    "        delay (float): Delay in seconds between batch uploads. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    collection = client.collections.get(collection_id)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Starting to add {len(doc_ids)} documents to collection '{collection_id}'\"\n",
    "    )\n",
    "    logger.debug(f\"Collection details: {collection}\")\n",
    "\n",
    "    batch_size = 200\n",
    "    for i in range(0, len(doc_ids), batch_size):\n",
    "        batch = doc_ids[i : i + batch_size]\n",
    "        logger.debug(\n",
    "            f\"Processing batch {i//batch_size + 1}: documents {i+1} to\"\n",
    "            f\" {min(i+batch_size, len(doc_ids))}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            client.collections.add_items(collection_id, batch)\n",
    "            logger.info(\n",
    "                f\"Successfully added {len(batch)} documents to collection\"\n",
    "                f\" '{collection_id}'\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding batch to collection: {e}\")\n",
    "            raise\n",
    "\n",
    "        if i + batch_size < len(doc_ids):  # Sleep only if there are more batches\n",
    "            logger.info(f\"Sleeping for {delay} seconds before adding the next batch...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Completed adding all {len(doc_ids)} documents to collection '{collection_id}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_collection_with_docs(\n",
    "    client,\n",
    "    name: str,\n",
    "    doc_ids: list[str],\n",
    "    description: str = \"\",\n",
    "    delay: float = 4,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a new collection and populate it with document IDs.\n",
    "\n",
    "    Args:\n",
    "        client (connect): Impresso API client.\n",
    "        name (str): Name of the new collection.\n",
    "        doc_ids (list[str]): List of document IDs to add to the collection.\n",
    "        description (str): Optional description of the collection.\n",
    "        delay (float): Delay in seconds between batch uploads. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        str: The ID of the created collection.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the collection ID is not found in the response.\n",
    "        Exception: If collection creation or document addition fails.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    try:\n",
    "        logger.info(\n",
    "            f\"Starting to create collection '{name}' with {len(doc_ids)} documents\"\n",
    "        )\n",
    "        logger.debug(f\"Collection description: {description}\")\n",
    "\n",
    "        collection = create_collection(client, name, description or \"\")\n",
    "        collection_id = collection.get(\"uid\")\n",
    "        if not collection_id:\n",
    "            raise ValueError(\"Collection ID not found in the response.\")\n",
    "\n",
    "        logger.info(f\"Collection '{name}' created with ID: {collection_id}\")\n",
    "        logger.debug(f\"Collection response: {collection}\")\n",
    "\n",
    "        time.sleep(delay)\n",
    "        add_docids(client, collection_id, doc_ids, delay)\n",
    "        logger.info(\n",
    "            f\"Successfully added {len(doc_ids)} documents to collection '{name}'\"\n",
    "        )\n",
    "\n",
    "        return collection_id\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create collection with docs: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's specify a name for the Impresso user collection\n",
    "\n",
    "Each collection must have a unique name. If you run this notebook multiple times, make sure to change the collection name to avoid conflicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"United Press International - Sampling 2025-07-08\"\n",
    "collection_description = (\n",
    "    \"Searching for the phrase 'United Press International' in the Impresso dataset.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = create_collection_with_docs(\n",
    "    client,\n",
    "    collection_name,\n",
    "    doc_ids,\n",
    "    collection_description,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes some articles get lost during collection building. This can happen due to\n",
    "various reasons, such as network issues or API limitations.\n",
    "It is absolutely save to rerun the function that adds content items without creating a\n",
    "new collection. If a content item is already in the collection, it will be skipped silently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docids(client, collection_id, doc_ids, delay=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the collection we just created. This will help us verify that the articles\n",
    "were added successfully and that the collection is ready for further analysis.\n",
    "There is also a link to the collection in the Impresso web interface, which you can use\n",
    "to view the articles directly and explore its facets and metadatda interactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = client.collections.get(collection_id)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the collection content with local document IDs\n",
    "\n",
    "To double-check if any and which articles were lost during the collection building process, we can compare the local document IDs with the actual content of the cloud collection.\n",
    "This will help us identify any discrepancies and ensure that the collection contains the expected articles.\n",
    "If there are any missing articles, we can re-run the function to add them to the collection.\n",
    "This is particularly useful for ensuring the integrity of the collection and verifying that all intended articles are included.\n",
    "This step is optional, but it can be helpful for debugging and ensuring the completeness of the collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_collection_content(\n",
    "    client: ImpressoClient, collection_id: str, local_doc_ids: list[str]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Compare local document IDs with the actual content of a cloud collection.\n",
    "\n",
    "    Args:\n",
    "        client (connect): Impresso API client.\n",
    "        collection_id (str): ID of the collection to compare against.\n",
    "        local_doc_ids (list[str]): List of document IDs that should be in the collection.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Dictionary containing:\n",
    "            - \"stored_docids\": List of document IDs that are successfully stored in the collection\n",
    "            - \"missing_docids\": List of document IDs that are missing from the collection\n",
    "\n",
    "    Raises:\n",
    "        Exception: If unable to retrieve collection content from the API.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logger.info(f\"Comparing local document list with collection '{collection_id}'\")\n",
    "    logger.debug(f\"Local document count: {len(local_doc_ids)}\")\n",
    "\n",
    "    try:\n",
    "        # Get the collection details\n",
    "        collection = client.collections.get(collection_id)\n",
    "        logger.debug(f\"Collection details: {collection}\")\n",
    "\n",
    "        # Get all document IDs from the collection\n",
    "        # Note: We need to fetch all items, so we use a high limit\n",
    "        collection_items = client.collections.items(collection_id, limit=10000).raw[\n",
    "            \"data\"\n",
    "        ]\n",
    "        print(collection_items)\n",
    "        stored_doc_ids = [\n",
    "            item.get(\"uid\") for item in collection_items if item.get(\"uid\")\n",
    "        ]\n",
    "\n",
    "        logger.info(f\"Retrieved {len(stored_doc_ids)} documents from collection\")\n",
    "        logger.debug(f\"First 10 stored document IDs: {stored_doc_ids[:10]}\")\n",
    "\n",
    "        # Convert to sets for efficient comparison\n",
    "        local_set = set(local_doc_ids)\n",
    "        stored_set = set(stored_doc_ids)\n",
    "\n",
    "        # Find intersection (stored) and difference (missing)\n",
    "        stored_docids = list(local_set.intersection(stored_set))\n",
    "        missing_docids = list(local_set.difference(stored_set))\n",
    "\n",
    "        logger.info(f\"Comparison results:\")\n",
    "        logger.info(f\"  - Documents successfully stored: {len(stored_docids)}\")\n",
    "        logger.info(f\"  - Documents missing from collection: {len(missing_docids)}\")\n",
    "\n",
    "        if missing_docids:\n",
    "            logger.warning(f\"Missing document IDs (first 10): {missing_docids[:10]}\")\n",
    "        else:\n",
    "            logger.info(\"All local documents are present in the collection!\")\n",
    "\n",
    "        return {\"stored_docids\": stored_docids, \"missing_docids\": missing_docids}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to compare collection content: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = compare_collection_content(client, collection_id, doc_ids)\n",
    "print(f\"Successfully stored: {len(comparison['stored_docids'])} documents\")\n",
    "print(f\"Missing from collection: {len(comparison['missing_docids'])} documents\")\n",
    "\n",
    "# If there are missing documents, you can add them\n",
    "if comparison[\"missing_docids\"]:\n",
    "    add_docids(client, collection_id, comparison[\"missing_docids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPQVeqFYmmLG"
   },
   "source": [
    "## Adding articles to an existing user collection\n",
    "\n",
    "If you already have a collection where you want to store additional articles, go to\n",
    "[your Impresso collection overview](https://impresso-project.ch/app/collections). Select the the collection that you\n",
    "want to add to. Look at the URL to find the technical unique collection id. It is\n",
    "everything after https://impresso-project.ch/app/collections/{collection_id}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, we have demonstrated how to perform stratified sampling of articles\n",
    "from the Impresso API and add them to a user collection. This process allows for\n",
    "efficient analysis of large newspaper archives by creating a representative subset of\n",
    "articles.\n",
    "\n",
    "You can now proceed to analyze the sampled articles in your collection.\n",
    "You can use the Impresso web interface to explore the collection, visualize the data,\n",
    "and perform further analyses using the Impresso API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
